{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57d4a577-fda0-46b8-9a7e-0cd745de34c3",
   "metadata": {},
   "source": [
    "# goal: Implement sentiment classifier using convolution neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4501c416c0f37998",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T08:25:52.428404274Z",
     "start_time": "2023-09-18T08:25:52.283408600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import your pytorch convolution tools\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9e80f3-448c-40cb-bde2-d05d7966eacd",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b421e392ed73c2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T10:30:12.950018167Z",
     "start_time": "2023-09-18T10:29:39.754627626Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# data loading\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import gensim.downloader as api\n",
    "\n",
    "# load gensim google vectors\n",
    "word_vectors = api.load('word2vec-google-news-300')\n",
    "\n",
    "\n",
    "def preprocess_data(filepath):\n",
    "    \"\"\"\n",
    "    load data from file. convert labels from string to numbers\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(filepath,skiprows=0)\n",
    "    # modify  dataset[1] such that positive = 1, negative=0\n",
    "    dataset[\"sentiment\"] = dataset[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenize_and_vectorize_sample(sample):\n",
    "    \"\"\"\n",
    "    takes text as input and return word vectors as output\n",
    "    \"\"\"\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "    tokens = tokenizer.tokenize(sample)\n",
    "    sample_vecs = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            sample_vecs.append(word_vectors[token])\n",
    "\n",
    "        except KeyError:\n",
    "            pass  # No matching token in the Google w2v vocab\n",
    "\n",
    "    return sample_vecs\n",
    "\n",
    "\n",
    "def pad_trunc(sample, max_len=400, embedding_dims=300):\n",
    "    \"\"\"\n",
    "    For a given sample pad with zero vectors or truncate to maxlen\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a vector of 0s the length of our word vectors\n",
    "    zero_vector = []\n",
    "    for _ in range(embedding_dims):\n",
    "        zero_vector.append(0.0)\n",
    "    if len(sample) > maxlen:\n",
    "        temp = sample[:maxlen]\n",
    "    elif len(sample) < maxlen:\n",
    "        temp = sample\n",
    "        # Append the appropriate number 0 vectors to the list\n",
    "        additional_elems = maxlen - len(sample)\n",
    "        for _ in range(additional_elems):\n",
    "            temp.append(zero_vector)\n",
    "    else:\n",
    "        temp = sample\n",
    "    return temp\n",
    "\n",
    "dataset = preprocess_data(\"data/IMDB_Dataset.csv\")\n",
    "# print(dataset.head())\n",
    "# vectorized_dataset = tokenize_and_vectorize_dataset(dataset) \n",
    "# print(vectorized_dataset.head(vectorized_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f8653-bace-46a9-b559-29f48e5f0a5e",
   "metadata": {},
   "source": [
    "# test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999c75c0f4613451",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "split_point = int(len(dataset)*.8)\n",
    "\n",
    "x_train = [sample[0] for i, sample in dataset.iloc[1:split_point,:].iterrows()]\n",
    "y_train = [sample[1] for i, sample in dataset.iloc[1:split_point,:].iterrows()]\n",
    "\n",
    "x_test = [sample[0] for i, sample in dataset.iloc[split_point:,:].iterrows()]\n",
    "y_test = [sample[1] for i, sample in dataset.iloc[split_point:,:].iterrows()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a7bc53-c9f1-4e0f-82dc-eeeab085fe7f",
   "metadata": {},
   "source": [
    "# analyze class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e008ec6d9d5721b5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(f\"training data: {Counter(y_train)}\")     \n",
    "print(f\"test data: {Counter(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e3a58-166d-4db5-a25e-6b89087e0f13",
   "metadata": {},
   "source": [
    "# batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a313e-6de7-4da7-b955-c0748c0ade31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(x_train, y_train, batch_size):\n",
    "    next_x_batch, next_y_batch = [], []\n",
    "    with tqdm(total=len(x_train), position=0, leave=True) as pbar:\n",
    "        for ip, output in zip(x_train, y_train):\n",
    "            next_x_batch.append(ip)\n",
    "            next_y_batch.append(output)\n",
    "            if len(next_x_batch) == batch_size:\n",
    "                yield next_x_batch, next_y_batch\n",
    "                next_x_batch, next_y_batch = [], []\n",
    "                pbar.update(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec34d7d0-e800-4e41-86e9-c1565ad404c7",
   "metadata": {},
   "source": [
    "# define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b413b-46ef-4232-a920-80cb8cb2d4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the Conv1D layer\n",
    "class Conv1DLayer(nn.Module):\n",
    "    def __init__(self, filters, kernel_size, input_channels, activation):\n",
    "        super(Conv1DLayer, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(input_channels, filters, kernel_size)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1d(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# Define the model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, filters, kernel_size, maxlen, embedding_dims, hidden_dims):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1d_layer = Conv1DLayer(filters, kernel_size, embedding_dims, nn.ReLU())\n",
    "        self.global_max_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.dense = nn.Linear(filters, hidden_dims)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(hidden_dims, 1)\n",
    "        self.activation2 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1d_layer(x)\n",
    "        x = self.global_max_pooling(x)\n",
    "        x = x.squeeze()\n",
    "        x = self.dense(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.activation2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b5a768-29ed-4bab-b589-dd405561a012",
   "metadata": {},
   "source": [
    "# model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d844ac63-ed71-46e2-ad68-62e78971ef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN parameters\n",
    "max_len = 400\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "# Instantiate the model\n",
    "maxlen = 400  # Example value for maxlen\n",
    "embedding_dims = 300  # Example value for embedding_dims\n",
    "hidden_dims = 250  # Example value for hidden_dims\n",
    "filters = 250  # Example value for filters\n",
    "kernel_size = 3  # Example value for kernel_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9256a184-2936-452d-b9fc-90ea6e34260c",
   "metadata": {},
   "source": [
    "# model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56d1444-8ed6-4c08-88e0-0aa7a4dccda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = Model(filters, kernel_size, maxlen, embedding_dims, hidden_dims)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = epochs  # Example value for epochs\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()     \n",
    "    loss_val = 0\n",
    "    for i, (x_batch, y_batch) in enumerate(generate_batch(x_train, y_train, batch_size=batch_size)):\n",
    "        # print(i)\n",
    "        x_batch = [tokenize_and_vectorize_sample(sample) for sample in x_batch]\n",
    "        x_batch = [pad_trunc(sample, max_len=max_len, embedding_dims=embedding_dims) for sample in x_batch]        \n",
    "        x_batch = Variable(torch.FloatTensor(x_batch))\n",
    "        x_batch = x_batch.permute(0, 2, 1)\n",
    "        # print(x_batch.shape)\n",
    "        y_batch = Variable(torch.FloatTensor([y_batch]))\n",
    "        y_batch = y_batch.reshape(batch_size,1)\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        loss_val += loss.item()\n",
    "        optimizer.step()\n",
    "    print(f\"epoch({epoch}): total_loss={loss_val}\")\n",
    "    loss_val=0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7c5f5b-532d-4b3e-a1c6-3a035efdbd13",
   "metadata": {
    "tags": []
   },
   "source": [
    "# save/load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf11dd4-b654-43cd-9e46-96d0f9091c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pytorch model\n",
    "# saving embeddings\n",
    "model_path = f\"imdb_cnn_model_{filters}_{kernel_size}_{maxlen}_{embedding_dims}_{hidden_dims}.pth\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1b6495-e5cc-433b-8463-7e8f7f230600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file\n",
    "model_path = f\"imdb_cnn_model_{filters}_{kernel_size}_{maxlen}_{embedding_dims}_{hidden_dims}.pth\"\n",
    "loaded_model = Model(filters, kernel_size, max_len, embedding_dims, hidden_dims)  # Create an instance of your model\n",
    "loaded_model.load_state_dict(torch.load(model_path))  # Load the state dictionary\n",
    "loaded_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Now 'loaded_model' contains the model loaded from the saved file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a351df67-879c-4568-a7d1-45a22743794b",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5373be-108c-4940-ba45-e9edb10e6b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the model and prepare input data (as shown in the previous responses)\n",
    "\n",
    "def evaluate(x_test, y_test, batch_size=32):\n",
    "    print(f\"len(x_test) == len(y_test): {len(x_test)} == {len(y_test)}\")\n",
    "    predictions = []\n",
    "    batches = int(len(x_test)/batch_size) +1\n",
    "    for i in tqdm(range(batches)):\n",
    "        x_batch = [tokenize_and_vectorize_sample(sample) for sample in x_test[i:i+batch_size]]        \n",
    "        x_batch = [pad_trunc(sample, max_len=max_len, embedding_dims=embedding_dims) for sample in x_batch]        \n",
    "        x_batch = Variable(torch.FloatTensor(x_batch))\n",
    "        x_batch = x_batch.permute(0, 2, 1)\n",
    "        y_batch = y_test[i:i+batch_size]\n",
    "        # print(x_batch.shape)\n",
    "        # Perform inference on the test data\n",
    "        with torch.no_grad():\n",
    "            # Forward pass to get predictions\n",
    "            batch_predictions = loaded_model(x_batch)\n",
    "            # Assuming 'predictions' is the model's predictions (binary values)\n",
    "            # print(predictions)\n",
    "            # Convert predictions to binary values based on a threshold (e.g., 0.5 for binary classification)\n",
    "            threshold = 0.5\n",
    "            binary_predictions = (batch_predictions > threshold).float()\n",
    "            predictions.extend(binary_predictions.squeeze().tolist())\n",
    "            # print(f\"len(binary_predictions):{len(binary_predictions)}\")\n",
    "    \n",
    "    total = min(len(y_test), len(predictions))\n",
    "    print(predictions[:total])\n",
    "    accuracy = accuracy_score(y_test[:total], predictions[:total])\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78545a6d-a8aa-467f-a5f2-c0cc4654ebb4",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82443479-f9ee-4660-8295-92787753f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text):\n",
    "    x_batch = [tokenize_and_vectorize_sample(sample) for sample in [text]]\n",
    "    x_batch = [pad_trunc(sample, max_len=max_len, embedding_dims=embedding_dims) for sample in x_batch]        \n",
    "    x_batch = Variable(torch.FloatTensor(x_batch))\n",
    "    x_batch = x_batch.permute(0, 2, 1)\n",
    "    print(x_batch.shape)\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        # Forward pass to get predictions\n",
    "        predictions = loaded_model(x_batch)\n",
    "        # If you're doing binary classification (as in your original Keras model)\n",
    "        # You may want to threshold the predictions to get the final classes\n",
    "        # Assuming a threshold of 0.5 for binary classification\n",
    "        print(predictions)\n",
    "        threshold = 0.5\n",
    "        binary_predictions = (predictions > threshold).float()\n",
    "        return binary_predictions\n",
    "\n",
    "review = inference(\"Movie was so goos that it was bad.\")\n",
    "print(review)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
