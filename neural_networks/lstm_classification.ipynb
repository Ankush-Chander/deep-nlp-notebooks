{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7555dc5f-32f8-4acf-8999-8ea1dfb5939d",
   "metadata": {},
   "source": [
    "# goal: Implement sentiment classifier using LSTM neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf31c4af-4f7b-4757-8164-6f28f90fcdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your pytorch tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# load gensim google vectors\n",
    "import gensim.downloader as api\n",
    "word_vectors = api.load('word2vec-google-news-300')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f214c9cc-2476-45d4-8214-a5ec9390730d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "device = get_default_device()\n",
    "print(device)\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352af132-c7b0-4fd4-9087-2aa9072854d0",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb54de4d-019a-4f63-98ad-6653095e0863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# data loading\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "\n",
    "# # load gensim google vectors\n",
    "# word_vectors = api.load('word2vec-google-news-300')\n",
    "\n",
    "\n",
    "def preprocess_data(filepath):\n",
    "    \"\"\"\n",
    "    load data from file. convert labels from string to numbers\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(filepath,skiprows=0)\n",
    "    # modify  dataset[1] such that positive = 1, negative=0\n",
    "    dataset[\"sentiment\"] = dataset[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenize_and_vectorize_sample(sample, max_len=400):\n",
    "    \"\"\"\n",
    "    takes text as input and return word vectors as output\n",
    "    \"\"\"\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "    tokens = tokenizer.tokenize(sample)\n",
    "    sample_vecs = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            sample_vecs.append(word_vectors[token])\n",
    "            if len(sample_vecs)>= max_len:\n",
    "                return sample_vecs\n",
    "            # print(f\"keeping: {token}\")\n",
    "        except KeyError:\n",
    "            # print(f\"skipping: {token}\")\n",
    "            pass  # No matching token in the Google w2v vocab\n",
    "\n",
    "    return sample_vecs\n",
    "\n",
    "\n",
    "dataset = preprocess_data(\"data/IMDB_Dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af898a1-a7b4-4bfb-afe1-eb7c844838df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0535fb14-9003-4d73-9043-a6cf36e695f1",
   "metadata": {},
   "source": [
    "# test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a832e5dd-d954-4b87-aaaa-bb6e5c13f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "split_point = int(len(dataset)*.8)\n",
    "\n",
    "x_train = [sample[0] for i, sample in dataset.iloc[1:split_point,:].iterrows()]\n",
    "y_train = [sample[1] for i, sample in dataset.iloc[1:split_point,:].iterrows()]\n",
    "\n",
    "x_test = [sample[0] for i, sample in dataset.iloc[split_point:,:].iterrows()]\n",
    "y_test = [sample[1] for i, sample in dataset.iloc[split_point:,:].iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f03e63de-0989-4426-83e4-10ae5e3f5d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n"
     ]
    }
   ],
   "source": [
    "# tokenize_and_vectorize_sample(x_train[0])\n",
    "print(len(x_train[0].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf6a8149-119c-4593-b125-10dabadc805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(x_train, y_train, batch_size):\n",
    "    next_x_batch, next_y_batch = [], []\n",
    "    with tqdm(total=len(x_train), position=0, leave=True) as pbar:\n",
    "        for ip, output in zip(x_train, y_train):\n",
    "            next_x_batch.append(ip)\n",
    "            next_y_batch.append(output)\n",
    "            if len(next_x_batch) == batch_size:\n",
    "                yield np.array(next_x_batch), np.array(next_y_batch)\n",
    "                next_x_batch, next_y_batch = [], []\n",
    "                pbar.update(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4145b86-92e7-4a8a-b2aa-7387cbbbcf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dims=300, hidden_dims=100, num_layers=1, batch_first=True):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dims,\n",
    "            hidden_size=hidden_dims,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_dims, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        r_out, h_n = self.lstm(x, None)\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15381fad-21fd-4580-8843-1d901bbea32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims=300\n",
    "hidden_dims= 100\n",
    "num_layers=2\n",
    "batch_first=True\n",
    "max_len=400\n",
    "epochs = 5\n",
    "batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be3d676c-a52d-470c-96ff-4dc877f9e749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 39999/39999 [06:53<00:00, 96.73it/s]\n",
      " 20%|███████████▊                                               | 1/5 [06:53<27:34, 413.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch(0): total_loss=15051.217386228702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 39999/39999 [06:58<00:00, 95.57it/s]\n",
      " 40%|███████████████████████▌                                   | 2/5 [13:52<20:49, 416.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch(1): total_loss=10540.264681527804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 39999/39999 [06:55<00:00, 96.27it/s]\n",
      " 60%|███████████████████████████████████▍                       | 3/5 [20:47<13:52, 416.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch(2): total_loss=8457.04387977241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 39999/39999 [06:54<00:00, 96.43it/s]\n",
      " 80%|███████████████████████████████████████████████▏           | 4/5 [27:42<06:55, 415.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch(3): total_loss=6404.521982997649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 39999/39999 [06:59<00:00, 95.25it/s]\n",
      "100%|███████████████████████████████████████████████████████████| 5/5 [34:42<00:00, 416.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch(4): total_loss=4649.106866779008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "lstm_model = LSTM(embedding_dims=300, hidden_dims=100, num_layers=num_layers, batch_first=True)\n",
    "lstm_model = to_device(lstm_model, device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters())\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = epochs  # Example value for epochs\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    lstm_model.train()     \n",
    "    loss_val = 0\n",
    "    for i, (x_batch, y_batch) in enumerate(generate_batch(x_train, y_train, batch_size=batch_size)):\n",
    "        # print(i)\n",
    "        x_batch = [tokenize_and_vectorize_sample(sample) for sample in x_batch]\n",
    "        x_batch = Variable(torch.FloatTensor(x_batch))\n",
    "        x_batch = to_device(x_batch, device)\n",
    "        # x_batch = x_batch.permute(0, 2, 1)\n",
    "        # print(x_batch.shape)\n",
    "        y_batch = to_device(Variable(torch.FloatTensor(np.array([y_batch]))), device)\n",
    "        y_batch = y_batch.reshape(batch_size,1)\n",
    "        outputs = lstm_model(x_batch)\n",
    "        # print(outputs)\n",
    "        # print(y_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        loss_val += loss.item()\n",
    "        optimizer.step()\n",
    "        # if i==20000:\n",
    "        #     break\n",
    "    print(f\"epoch({epoch}): total_loss={loss_val}\")\n",
    "    loss_val=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ccb21cd-7a61-443f-b35e-31bc685db0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pytorch model\n",
    "# saving embeddings\n",
    "model_path = f\"imdb_lstm_model_{max_len}_{embedding_dims}_{hidden_dims}_{num_layers}.pth\"\n",
    "torch.save(lstm_model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6aac50f-1d68-4d84-bb03-c0f46a2f2004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file\n",
    "model_path = f\"imdb_lstm_model_{max_len}_{embedding_dims}_{hidden_dims}_{num_layers}.pth\"\n",
    "loaded_model = LSTM(embedding_dims=embedding_dims, hidden_dims=hidden_dims, num_layers=num_layers)  # Create an instance of your model\n",
    "loaded_model.load_state_dict(torch.load(model_path))  # Load the state dictionary\n",
    "loaded_model.eval()  # Set the model to evaluation mode\n",
    "loaded_model = to_device(loaded_model, device)\n",
    "# Now 'loaded_model' contains the model loaded from the saved file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8392c5-98e4-4c41-bc04-490008c0f05d",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09f44a98-bf5f-4194-8cce-727d0e55f0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(x_test) == len(y_test): 10000 == 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████▉| 10000/10001 [01:03<00:00, 158.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8863\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the model and prepare input data (as shown in the previous responses)\n",
    "\n",
    "def evaluate(x_test, y_test, batch_size=1):\n",
    "    print(f\"len(x_test) == len(y_test): {len(x_test)} == {len(y_test)}\")\n",
    "    predictions = []\n",
    "    batches = int(len(x_test)/batch_size) +1\n",
    "    for i in tqdm(range(batches)):\n",
    "        x_batch = x_test[i:i+batch_size]\n",
    "        y_batch = y_test[i:i+batch_size]\n",
    "        \n",
    "        if not x_batch or not y_batch:\n",
    "            break\n",
    "        \n",
    "        x_batch = [tokenize_and_vectorize_sample(sample, max_len=max_len) for sample in x_batch]\n",
    "        \n",
    "        x_batch = Variable(torch.FloatTensor(x_batch))\n",
    "        x_batch = to_device(x_batch, device)\n",
    "        \n",
    "        \n",
    "        y_batch = to_device(Variable(torch.FloatTensor([y_batch])), device)\n",
    "        y_batch = y_batch.reshape(batch_size,1)\n",
    "        y_batch = to_device(y_batch, device)\n",
    "        # print(x_batch.shape)\n",
    "        # Perform inference on the test data\n",
    "        with torch.no_grad():\n",
    "            # Forward pass to get predictions\n",
    "            batch_predictions = loaded_model(x_batch)\n",
    "            # print(f\"batch_predictions: {batch_predictions}\")\n",
    "            # Assuming 'predictions' is the model's predictions (binary values)\n",
    "            # print(predictions)\n",
    "            # Convert predictions to binary values based on a threshold (e.g., 0.5 for binary classification)\n",
    "            threshold = 0.5\n",
    "            binary_predictions = (batch_predictions > threshold).float()\n",
    "            binary_predictions = [bp.squeeze(0).cpu() for bp in binary_predictions]\n",
    "            # print(f\"binary_predictions.squeeze(): {binary_predictions.squeeze()}\")\n",
    "            predictions.extend(binary_predictions)\n",
    "            \n",
    "            # print(f\"len(binary_predictions):{len(binary_predictions)}\")\n",
    "    \n",
    "    total = min(len(y_test), len(predictions))\n",
    "    # print(y_test[:total])\n",
    "    # print(predictions[:total])\n",
    "    accuracy = accuracy_score(y_test[:total], predictions[:total])\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "evaluate(x_test, y_test)\n",
    "# print(type(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62b1a71-1daa-4176-a91a-bf7fbcdc043b",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9177c25b-793d-4fe5-b02b-c32a4c6bdb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def inference(text):\n",
    "    x_batch = [tokenize_and_vectorize_sample(sample, max_len=max_len) for sample in [text]]\n",
    "    # x_batch = [pad_trunc(sample, max_len=max_len, embedding_dims=embedding_dims) for sample in x_batch]        \n",
    "    x_batch = to_device(Variable(torch.FloatTensor(x_batch)),device)\n",
    "    # x_batch = x_batch.permute(0, 2, 1)\n",
    "    # print(x_batch.shape)\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        # Forward pass to get predictions\n",
    "        predictions = loaded_model(x_batch)\n",
    "        # If you're doing binary classification (as in your original Keras model)\n",
    "        # You may want to threshold the predictions to get the final classes\n",
    "        # Assuming a threshold of 0.5 for binary classification\n",
    "        # print(predictions)\n",
    "        threshold = 0.5\n",
    "        binary_predictions = (predictions > threshold).float()\n",
    "        return binary_predictions\n",
    "\n",
    "review = inference(\"\"\"Movie was good.\"\"\")\n",
    "\n",
    "print(review)\n",
    "\n",
    "# print(x_train[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
