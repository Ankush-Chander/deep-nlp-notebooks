{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7555dc5f-32f8-4acf-8999-8ea1dfb5939d",
   "metadata": {},
   "source": [
    "# goal: Implement sentiment classifier using LSTM neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf31c4af-4f7b-4757-8164-6f28f90fcdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your pytorch tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# load gensim google vectors\n",
    "import gensim.downloader as api\n",
    "word_vectors = api.load('word2vec-google-news-300')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f214c9cc-2476-45d4-8214-a5ec9390730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "device = get_default_device()\n",
    "print(device)\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8f2626-e856-4fd9-b611-ad6364abe6a6",
   "metadata": {},
   "source": [
    "# data download\n",
    "\n",
    "Download data from [IMDB Dataset of 50K Movie Reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352af132-c7b0-4fd4-9087-2aa9072854d0",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb54de4d-019a-4f63-98ad-6653095e0863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# data loading\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "\n",
    "# # load gensim google vectors\n",
    "# word_vectors = api.load('word2vec-google-news-300')\n",
    "\n",
    "\n",
    "def preprocess_data(filepath):\n",
    "    \"\"\"\n",
    "    load data from file. convert labels from string to numbers\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(filepath,skiprows=0)\n",
    "    # modify  dataset[1] such that positive = 1, negative=0\n",
    "    dataset[\"sentiment\"] = dataset[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenize_and_vectorize_sample(sample, max_len=400):\n",
    "    \"\"\"\n",
    "    takes text as input and return word vectors as output\n",
    "    \"\"\"\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "    tokens = tokenizer.tokenize(sample)\n",
    "    sample_word_vecs = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            sample_word_vecs.append((token, word_vectors[token]))\n",
    "            if len(sample_word_vecs)>= max_len:\n",
    "                return sample_word_vecs\n",
    "            # print(f\"keeping: {token}\")\n",
    "        except KeyError:\n",
    "            # print(f\"skipping: {token}\")\n",
    "            pass  # No matching token in the Google w2v vocab\n",
    "    # print(f\"sample_words: {sample_words}\")\n",
    "    return sample_word_vecs\n",
    "\n",
    "\n",
    "dataset = preprocess_data(\"data/IMDB_Dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af898a1-a7b4-4bfb-afe1-eb7c844838df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0535fb14-9003-4d73-9043-a6cf36e695f1",
   "metadata": {},
   "source": [
    "# test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a832e5dd-d954-4b87-aaaa-bb6e5c13f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "split_point = int(len(dataset)*.8)\n",
    "\n",
    "x_train = [sample[0] for i, sample in dataset.iloc[1:split_point,:].iterrows()]\n",
    "y_train = [sample[1] for i, sample in dataset.iloc[1:split_point,:].iterrows()]\n",
    "\n",
    "x_test = [sample[0] for i, sample in dataset.iloc[split_point:,:].iterrows()]\n",
    "y_test = [sample[1] for i, sample in dataset.iloc[split_point:,:].iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03e63de-0989-4426-83e4-10ae5e3f5d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize_and_vectorize_sample(x_train[0])\n",
    "print(len(x_train[0].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6a8149-119c-4593-b125-10dabadc805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(x_train, y_train, batch_size):\n",
    "    next_x_batch, next_y_batch = [], []\n",
    "    with tqdm(total=len(x_train), position=0, leave=True) as pbar:\n",
    "        for ip, output in zip(x_train, y_train):\n",
    "            next_x_batch.append(ip)\n",
    "            next_y_batch.append(output)\n",
    "            if len(next_x_batch) == batch_size:\n",
    "                yield np.array(next_x_batch), np.array(next_y_batch)\n",
    "                next_x_batch, next_y_batch = [], []\n",
    "                pbar.update(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4145b86-92e7-4a8a-b2aa-7387cbbbcf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dims=300, hidden_dims=100, num_layers=1, batch_first=True):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dims,\n",
    "            hidden_size=hidden_dims,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_dims, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        r_out, h_n = self.lstm(x, None)\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae5cd40-6e0d-4b30-aa2a-0e693c26e04d",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "1. Find out words closest to the final state\n",
    "2. Find out words that displace history the most\n",
    "3. Find out words that are most displaced from their default position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2770ebd1-2cb6-403a-abe3-1d097b232b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition used only while inferencing for experimentation\n",
    "import traceback\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_top_k_scores(score_tuples, k=10, key=-2, y_label=\"scores\"):\n",
    "    \"\"\"\n",
    "    plot a bar chart displaying top k words and corresponding scores\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    words = [tup[0] for tup in score_tuples[:k]]\n",
    "    scores = [tup[key] for tup in score_tuples[:k]]\n",
    "    ax.bar(words, scores)\n",
    "    ax.set_ylabel(y_label)\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=10, rotation='vertical')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def exp_1(func):\n",
    "    \"\"\"\n",
    "    experiment: Find out words closest to the final state\n",
    "    goal: calculate distance between each default word representation and final output\n",
    "    assumption: final output should land closer to supporting adjectives and away from non supporting adjectives\n",
    "    \"\"\"\n",
    "    def wrapped(self, *args, **kwargs):\n",
    "        # print('inside decorator.')\n",
    "        out, r_out = func(self, *args, **kwargs)\n",
    "        \n",
    "        # print(f\"r_out.shape: {r_out.shape}\")\n",
    "        \n",
    "        try:\n",
    "            text = kwargs.get(\"text\", None)\n",
    "            x_batch = kwargs.get(\"x_batch\", None)            \n",
    "            # convert text to x_batch\n",
    "            if not x_batch and text:\n",
    "                sample_word_vecs = [tokenize_and_vectorize_sample(sample, max_len=max_len) for sample in [text]]\n",
    "                word_list = [word for sample in sample_word_vecs for word, *_ in sample]\n",
    "                x_wordvec_list = [vec for sample in sample_word_vecs for *_, vec in sample]\n",
    "                x_batch = np.array([x_wordvec_list])\n",
    "                # x_batch = [pad_trunc(sample, max_len=max_len, embedding_dims=embedding_dims) for sample in x_batch]        \n",
    "                x_batch = to_device(Variable(torch.FloatTensor(x_batch)), device)\n",
    "            # print(f\"x_batch.shape:{x_batch.shape}\")\n",
    "            \n",
    "            ###### begin experimental\n",
    "            # calculate effect of each word without history\n",
    "            # print(f\"x_batch.shape={x_batch.shape}\")\n",
    "            r_singles_hns = [self.lstm(word.unsqueeze(0), None) for word in x_batch.squeeze(0).squeeze(0)]        \n",
    "            # print(f\"r_out.shape: {r_out.shape}\")        \n",
    "            dot_products = [torch.dot(r_singles.squeeze(0),r_out[:,-1, :].squeeze(0)).item() for r_singles,*_ in r_singles_hns]\n",
    "            # calculate cosine similarity of final snowball with each historyless word effect\n",
    "            cos = nn.CosineSimilarity(dim=0, eps=1e-6)        \n",
    "            cos_similarities = [cos(r_singles.squeeze(0),r_out[:,-1, :].squeeze(0)).item() for r_singles,*_ in r_singles_hns]\n",
    "\n",
    "            # combine position, word, scores for logging purpose\n",
    "            # word_idx  = [idx for idx,*_ in enumerate(word_list)]\n",
    "            word_scores = list(set((zip(word_list, dot_products, cos_similarities))))\n",
    "            word_scores = sorted(word_scores, key=lambda x:-x[-2])\n",
    "            \n",
    "            # plot top 10\n",
    "            plot_top_k_scores(word_scores,k=10,key=-2,  y_label=\"word_scores\")\n",
    "            word_scores = '\\n'.join([str(ws) for ws in word_scores])\n",
    "            print(text)\n",
    "            print(f\"word_scores: {(word_scores)}\")\n",
    "            \n",
    "        except Exception as err:\n",
    "            print(traceback.format_exc())\n",
    "        # print(f\"exiting wrapped\")\n",
    "        return out, r_out \n",
    "    return wrapped\n",
    "\n",
    "\n",
    "def exp_2(func):\n",
    "    \"\"\"\n",
    "    Goal: How much snowball at timestamp t is moved from the word at timestep t\n",
    "    Experiment: calculate force exerted by history on each word\n",
    "    assumption: supporting words will be displaced less, opposing words will be displaced more by the history. Displacement means distance between input word and output of a cell\n",
    "    findings: ?\n",
    "    \"\"\"\n",
    "    def wrapped(self, *args, **kwargs):\n",
    "        # print('inside decorator.')\n",
    "        out, r_out = func(self, *args, **kwargs)\n",
    "        \n",
    "\n",
    "        try:\n",
    "            text = kwargs.get(\"text\", None)\n",
    "            x_batch = kwargs.get(\"x_batch\", None)            \n",
    "            # convert text to x_batch\n",
    "            if not x_batch and text:\n",
    "                sample_word_vecs = [tokenize_and_vectorize_sample(sample, max_len=max_len) for sample in [text]]\n",
    "                word_list = [word for sample in sample_word_vecs for word, *_ in sample]\n",
    "                x_wordvec_list = [vec for sample in sample_word_vecs for *_, vec in sample]\n",
    "                x_batch = np.array([x_wordvec_list])\n",
    "                # x_batch = [pad_trunc(sample, max_len=max_len, embedding_dims=embedding_dims) for sample in x_batch]        \n",
    "                x_batch = to_device(Variable(torch.FloatTensor(x_batch)), device)\n",
    "                        \n",
    "            # calculate representation of each word without history\n",
    "            r_singles_hns = [self.lstm(word.unsqueeze(0), None) for word in x_batch.squeeze(0).squeeze(0)]        \n",
    "            \n",
    "            \n",
    "            dot_products = [torch.dot(r_singles.squeeze(0),r_out[:,i, :].squeeze(0)).item() for i, (r_singles,*_) in enumerate(r_singles_hns)]\n",
    "            \n",
    "            # calculate cosine similarity of snowball at timestamp t with each historyless word effect\n",
    "            cos = nn.CosineSimilarity(dim=0, eps=1e-6)        \n",
    "            cos_similarities = [cos(r_singles.squeeze(0),r_out[:,i, :].squeeze(0)).item() for i, (r_singles,*_) in enumerate(r_singles_hns)]\n",
    "\n",
    "            # combine position, word, scores for logging purpose\n",
    "            word_displacements = list(set((zip(word_list, dot_products, cos_similarities))))\n",
    "            word_displacements = sorted(word_displacements, key=lambda x:-x[-2])\n",
    "            \n",
    "            # plot top 10\n",
    "            plot_top_k_scores(word_displacements,k=10,key=-2, y_label=\"word_displacements\")\n",
    "            \n",
    "            word_displacements = '\\n'.join([str(wd) for wd in word_displacements])\n",
    "            print(text)\n",
    "            print(f\"word_displacements: {(word_displacements)}\")\n",
    "\n",
    "            # end experimental\n",
    "        except Exception as err:\n",
    "            print(traceback.format_exc())\n",
    "        \n",
    "        return out, r_out \n",
    "    return wrapped\n",
    "\n",
    "\n",
    "def exp_3(func):\n",
    "    \"\"\"\n",
    "    Goal: Calculate which words(enforcers) lead to maximum change in cell output/snowball\n",
    "    assumption: supporting words will be displaced less, opposing words will be displaced more by the history. Displacement means distance between input word and output of a cell\n",
    "    findings: ?  \n",
    "    \"\"\"\n",
    "    def wrapped(self, *args, **kwargs):\n",
    "        out, r_out = func(self, *args, **kwargs)\n",
    "        \n",
    "        try:\n",
    "            text = kwargs.get(\"text\", None)\n",
    "            x_batch = kwargs.get(\"x_batch\", None)            \n",
    "            # convert text to x_batch\n",
    "            if not x_batch and text:\n",
    "                sample_word_vecs = [tokenize_and_vectorize_sample(sample, max_len=max_len) for sample in [text]]\n",
    "                word_list = [word for sample in sample_word_vecs for word, *_ in sample]\n",
    "                x_wordvec_list = [vec for sample in sample_word_vecs for *_, vec in sample]\n",
    "                x_batch = np.array([x_wordvec_list])\n",
    "                # x_batch = [pad_trunc(sample, max_len=max_len, embedding_dims=embedding_dims) for sample in x_batch]        \n",
    "                x_batch = to_device(Variable(torch.FloatTensor(x_batch)), device)\n",
    "            \n",
    "            total_words = len(list(x_batch.squeeze(0).squeeze(0))\n",
    "                              \n",
    "            dot_products = [torch.dot(r_out[:,i, :].squeeze(0), r_out[:,i+1, :].squeeze(0)).item() for i in range(total_words-1)]\n",
    "            \n",
    "            # calculate cosine similarity between two adjacent cell outputs\n",
    "            cos = nn.CosineSimilarity(dim=0, eps=1e-6)        \n",
    "            cos_similarities = [cos(r_out[:,i, :].squeeze(0), r_out[:,i+1, :].squeeze(0)).item() for i in range(total_words-1)]\n",
    "\n",
    "            # combine position, word, scores for logging purpose\n",
    "            # word_idx  = [idx for idx,*_ in enumerate(word_list)]\n",
    "            history_displacements = list(set((zip(word_list[1:], dot_products, cos_similarities))))\n",
    "            history_displacements = sorted(history_displacements, key=lambda x:-x[-2])\n",
    "            \n",
    "            plot_top_k_scores(history_displacements,k=10,key=-2, y_label=\"history_displacements\")\n",
    "            \n",
    "            history_displacements = '\\n'.join([str(wd) for wd in history_displacements])\n",
    "            print(text)\n",
    "            print(f\"history_displacements: {(history_displacements)}\")\n",
    "\n",
    "            # end experimental\n",
    "        except Exception as err:\n",
    "            print(traceback.format_exc())\n",
    "        return out, r_out \n",
    "    return wrapped\n",
    "\n",
    "\n",
    "class ExperimentalLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dims=300, hidden_dims=100, num_layers=1, batch_first=True):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dims,\n",
    "            hidden_size=hidden_dims,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Linear(hidden_dims, 1)\n",
    "        \n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    \n",
    "    # use decorator for experimentation during inference \n",
    "    @exp_3\n",
    "    def forward(self, text:str=\"\", x_batch=None):\n",
    "        #1.  get words,vecs from text\n",
    "        \n",
    "        #TODO: avoid repetetion\n",
    "        #if True: #not x_batch and text:            \n",
    "        sample_word_vecs = [tokenize_and_vectorize_sample(sample, max_len=max_len) for sample in [text]]\n",
    "        word_list = [word for sample in sample_word_vecs for word, *_ in sample]\n",
    "        x_wordvec_list = [vec for sample in sample_word_vecs for *_, vec in sample]\n",
    "        \n",
    "        x_batch = np.array([x_wordvec_list])\n",
    "        # x_batch = [pad_trunc(sample, max_len=max_len, embedding_dims=embedding_dims) for sample in x_batch]        \n",
    "        x_batch = to_device(Variable(torch.FloatTensor(x_batch)),device)\n",
    "        \n",
    "        # print(f\"calling self.lstm\")\n",
    "        r_out, h_n = self.lstm(x_batch, None)\n",
    "        print(f\"r_out.shape:{r_out.shape}\")\n",
    "\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        out = self.activation(out)\n",
    "        return out, r_out #final outputs and intermediate histories(r_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15381fad-21fd-4580-8843-1d901bbea32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims=300\n",
    "hidden_dims= 100\n",
    "num_layers=2\n",
    "batch_first=True\n",
    "max_len=400\n",
    "epochs = 5\n",
    "batch_size=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d878520f-99ab-4f9c-bc89-e98552ef5ecc",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb00130-28e1-41a6-be3f-915b50e32047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "\n",
    "lstm_model = LSTM(embedding_dims=300, hidden_dims=100, num_layers=num_layers, batch_first=True)\n",
    "lstm_model = to_device(lstm_model, device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters())\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = epochs  # Example value for epochs\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    lstm_model.train()     \n",
    "    loss_val = 0\n",
    "    for i, (x_batch, y_batch) in enumerate(generate_batch(x_train, y_train, batch_size=batch_size)):\n",
    "        # print(i)\n",
    "        x_batch = [tokenize_and_vectorize_sample(sample) for sample in x_batch]\n",
    "        x_batch = Variable(torch.FloatTensor(x_batch))\n",
    "        x_batch = to_device(x_batch, device)\n",
    "        # x_batch = x_batch.permute(0, 2, 1)\n",
    "        # print(x_batch.shape)\n",
    "        y_batch = to_device(Variable(torch.FloatTensor(np.array([y_batch]))), device)\n",
    "        y_batch = y_batch.reshape(batch_size,1)\n",
    "        outputs = lstm_model(x_batch)\n",
    "        # print(outputs)\n",
    "        # print(y_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        loss_val += loss.item()\n",
    "        optimizer.step()\n",
    "        # if i==20000:\n",
    "        #     break\n",
    "    print(f\"epoch({epoch}): total_loss={loss_val}\")\n",
    "    loss_val=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2079e2-e3d1-4c00-9cd6-cf6993aae4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pytorch model\n",
    "# saving embeddings\n",
    "model_path = f\"imdb_lstm_model_{max_len}_{embedding_dims}_{hidden_dims}_{num_layers}.pth\"\n",
    "torch.save(lstm_model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aac50f-1d68-4d84-bb03-c0f46a2f2004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file\n",
    "model_path = f\"imdb_lstm_model_{max_len}_{embedding_dims}_{hidden_dims}_{num_layers}.pth\"\n",
    "loaded_model = ExperimentalLSTM(embedding_dims=embedding_dims, hidden_dims=hidden_dims, num_layers=num_layers)  # Create an instance of your model\n",
    "loaded_model.load_state_dict(torch.load(model_path))  # Load the state dictionary\n",
    "loaded_model.eval()  # Set the model to evaluation mode\n",
    "loaded_model = to_device(loaded_model, device)\n",
    "# Now 'loaded_model' contains the model loaded from the saved file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8392c5-98e4-4c41-bc04-490008c0f05d",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57945cf-feb3-4c95-967e-caeda57fa535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the model and prepare input data (as shown in the previous responses)\n",
    "\n",
    "def evaluate(x_test, y_test, batch_size=1):\n",
    "    print(f\"len(x_test) == len(y_test): {len(x_test)} == {len(y_test)}\")\n",
    "    predictions = []\n",
    "    batches = int(len(x_test)/batch_size) +1\n",
    "    for i in tqdm(range(batches)):\n",
    "        x_batch = x_test[i:i+batch_size]\n",
    "        y_batch = y_test[i:i+batch_size]\n",
    "        \n",
    "        if not x_batch or not y_batch:\n",
    "            break\n",
    "        \n",
    "        x_batch = [tokenize_and_vectorize_sample(sample, max_len=max_len) for sample in x_batch]\n",
    "        \n",
    "        x_batch = Variable(torch.FloatTensor(x_batch))\n",
    "        x_batch = to_device(x_batch, device)\n",
    "        \n",
    "        \n",
    "        y_batch = to_device(Variable(torch.FloatTensor([y_batch])), device)\n",
    "        y_batch = y_batch.reshape(batch_size,1)\n",
    "        y_batch = to_device(y_batch, device)\n",
    "        # print(x_batch.shape)\n",
    "        # Perform inference on the test data\n",
    "        with torch.no_grad():\n",
    "            # Forward pass to get predictions\n",
    "            batch_predictions = loaded_model(x_batch)\n",
    "            # print(f\"batch_predictions: {batch_predictions}\")\n",
    "            # Assuming 'predictions' is the model's predictions (binary values)\n",
    "            # print(predictions)\n",
    "            # Convert predictions to binary values based on a threshold (e.g., 0.5 for binary classification)\n",
    "            threshold = 0.5\n",
    "            binary_predictions = (batch_predictions > threshold).float()\n",
    "            binary_predictions = [bp.squeeze(0).cpu() for bp in binary_predictions]\n",
    "            # print(f\"binary_predictions.squeeze(): {binary_predictions.squeeze()}\")\n",
    "            predictions.extend(binary_predictions)\n",
    "            \n",
    "            # print(f\"len(binary_predictions):{len(binary_predictions)}\")\n",
    "    \n",
    "    total = min(len(y_test), len(predictions))\n",
    "    # print(y_test[:total])\n",
    "    # print(predictions[:total])\n",
    "    accuracy = accuracy_score(y_test[:total], predictions[:total])\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "evaluate(x_test, y_test)\n",
    "# print(type(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62b1a71-1daa-4176-a91a-bf7fbcdc043b",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34293b0-447d-4af6-b8cb-a3c84fb60847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text):\n",
    "    sample_word_vecs = [tokenize_and_vectorize_sample(sample, max_len=max_len) for sample in [text]]\n",
    "    word_list = [word for sample in sample_word_vecs for word, *_ in sample]\n",
    "    x_wordvec_list = [vec for sample in sample_word_vecs for *_, vec in sample]\n",
    "    print(f\"len(x_wordvec_list): {len(x_wordvec_list)}\")\n",
    "    print(f\"type(x_wordvec_list[0]): {type(x_wordvec_list[0])}\")\n",
    "    print(word_list)\n",
    "    x_batch = np.array([x_wordvec_list])\n",
    "    # x_batch = [pad_trunc(sample, max_len=max_len, embedding_dims=embedding_dims) for sample in x_batch]        \n",
    "    x_batch = to_device(Variable(torch.FloatTensor(x_batch)),device)\n",
    "    # x_batch = x_batch.permute(0, 2, 1)\n",
    "    # print(x_batch.shape)\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        # Forward pass to get predictions\n",
    "        predictions = loaded_model(x_batch)\n",
    "        # If you're doing binary classification (as in your original Keras model)\n",
    "        # You may want to threshold the predictions to get the final classes\n",
    "        # Assuming a threshold of 0.5 for binary classification\n",
    "        # print(predictions)\n",
    "        threshold = 0.5\n",
    "        binary_predictions = (predictions > threshold).float()\n",
    "        return binary_predictions\n",
    "\n",
    "review = inference(\"\"\"Movie was good and it felt like a good movie\"\"\")\n",
    "\n",
    "print(review)\n",
    "\n",
    "# print(x_train[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce75a56-c4be-49ce-b141-0ba8a68ebd1e",
   "metadata": {},
   "source": [
    "# Experimental inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8d37dd-6cab-4953-b318-d694addc8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "from termcolor import colored\n",
    "import re\n",
    "def experimental_inference(text):\n",
    "    # x_batch = x_batch.permute(0, 2, 1)\n",
    "    # print(x_batch.shape)\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        # Forward pass to get predictions\n",
    "        predictions, *_ = loaded_model(text=text, x_batch=None)\n",
    "        # If you're doing binary classification (as in your original Keras model)\n",
    "        # You may want to threshold the predictions to get the final classes\n",
    "        # Assuming a threshold of 0.5 for binary classification\n",
    "        print(predictions)\n",
    "        threshold = 0.5\n",
    "        binary_predictions = (predictions > threshold).float()\n",
    "        return binary_predictions\n",
    "\n",
    "# print(x_test[1])\n",
    "texts = [\n",
    "    \"movie was terrible and acting was great.\",\n",
    "    \"acting was great and movie was terrible.\"\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "sample_id = randrange(len(x_train))\n",
    "print(f\"sample_id:{sample_id}\")\n",
    "\n",
    "for text in x_train[sample_id:sample_id+1]:\n",
    "    try:\n",
    "        # print(f\"{text}\")\n",
    "        print(f\"actual:{y_train[sample_id:sample_id+1]}\")        \n",
    "        text = re.sub('<[^<]+?>', '', text)\n",
    "        review = experimental_inference(text)        \n",
    "        print(f\"predicted: {review}\")        \n",
    "        print(\"=\"*100)\n",
    "    except Exception as err:\n",
    "        pass\n",
    "    \n",
    "# \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
