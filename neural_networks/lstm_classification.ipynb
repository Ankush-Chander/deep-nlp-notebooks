{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7555dc5f-32f8-4acf-8999-8ea1dfb5939d",
   "metadata": {},
   "source": [
    "# goal: Implement sentiment classifier using LSTM neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf31c4af-4f7b-4757-8164-6f28f90fcdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your pytorch tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# load gensim google vectors\n",
    "import gensim.downloader as api\n",
    "word_vectors = api.load('word2vec-google-news-300')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f214c9cc-2476-45d4-8214-a5ec9390730d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "device = get_default_device()\n",
    "print(device)\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352af132-c7b0-4fd4-9087-2aa9072854d0",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb54de4d-019a-4f63-98ad-6653095e0863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# data loading\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "\n",
    "# # load gensim google vectors\n",
    "# word_vectors = api.load('word2vec-google-news-300')\n",
    "\n",
    "\n",
    "def preprocess_data(filepath):\n",
    "    \"\"\"\n",
    "    load data from file. convert labels from string to numbers\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(filepath,skiprows=0)\n",
    "    # modify  dataset[1] such that positive = 1, negative=0\n",
    "    dataset[\"sentiment\"] = dataset[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenize_and_vectorize_sample(sample, max_len=400):\n",
    "    \"\"\"\n",
    "    takes text as input and return word vectors as output\n",
    "    \"\"\"\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "    tokens = tokenizer.tokenize(sample)\n",
    "    sample_word_vecs = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            sample_word_vecs.append((token, word_vectors[token]))\n",
    "            if len(sample_word_vecs)>= max_len:\n",
    "                return sample_word_vecs\n",
    "            # print(f\"keeping: {token}\")\n",
    "        except KeyError:\n",
    "            # print(f\"skipping: {token}\")\n",
    "            pass  # No matching token in the Google w2v vocab\n",
    "    # print(f\"sample_words: {sample_words}\")\n",
    "    return sample_word_vecs\n",
    "\n",
    "\n",
    "dataset = preprocess_data(\"data/IMDB_Dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af898a1-a7b4-4bfb-afe1-eb7c844838df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0535fb14-9003-4d73-9043-a6cf36e695f1",
   "metadata": {},
   "source": [
    "# test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a832e5dd-d954-4b87-aaaa-bb6e5c13f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "split_point = int(len(dataset)*.8)\n",
    "\n",
    "x_train = [sample[0] for i, sample in dataset.iloc[1:split_point,:].iterrows()]\n",
    "y_train = [sample[1] for i, sample in dataset.iloc[1:split_point,:].iterrows()]\n",
    "\n",
    "x_test = [sample[0] for i, sample in dataset.iloc[split_point:,:].iterrows()]\n",
    "y_test = [sample[1] for i, sample in dataset.iloc[split_point:,:].iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f03e63de-0989-4426-83e4-10ae5e3f5d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n"
     ]
    }
   ],
   "source": [
    "# tokenize_and_vectorize_sample(x_train[0])\n",
    "print(len(x_train[0].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf6a8149-119c-4593-b125-10dabadc805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(x_train, y_train, batch_size):\n",
    "    next_x_batch, next_y_batch = [], []\n",
    "    with tqdm(total=len(x_train), position=0, leave=True) as pbar:\n",
    "        for ip, output in zip(x_train, y_train):\n",
    "            next_x_batch.append(ip)\n",
    "            next_y_batch.append(output)\n",
    "            if len(next_x_batch) == batch_size:\n",
    "                yield np.array(next_x_batch), np.array(next_y_batch)\n",
    "                next_x_batch, next_y_batch = [], []\n",
    "                pbar.update(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4145b86-92e7-4a8a-b2aa-7387cbbbcf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dims=300, hidden_dims=100, num_layers=1, batch_first=True):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dims,\n",
    "            hidden_size=hidden_dims,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_dims, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        r_out, h_n = self.lstm(x, None)\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae5cd40-6e0d-4b30-aa2a-0e693c26e04d",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "1. Find out words closest to the final state\n",
    "2. Find out words that displace history the most\n",
    "3. Find out words that are most displaced from their default position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2770ebd1-2cb6-403a-abe3-1d097b232b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition used only while inferencing for experimentation\n",
    "import traceback\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_top_k_scores(score_tuples, k=10, key=-2, y_label=\"scores\"):\n",
    "    \"\"\"\n",
    "    plot a bar chart displaying top k words and corresponding scores\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    words = [tup[0] for tup in score_tuples[:k]]\n",
    "    scores = [tup[key] for tup in score_tuples[:k]]\n",
    "    ax.bar(words, scores)\n",
    "    ax.set_ylabel(y_label)\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=10, rotation='vertical')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def exp_1(func):\n",
    "    \"\"\"\n",
    "    experiment: Find out words closest to the final state\n",
    "    goal: calculate distance between each default word representation and final output\n",
    "    assumption: final output should land closer to supporting adjectives and away from non supporting adjectives\n",
    "    \"\"\"\n",
    "    def wrapped(self, *args, **kwargs):\n",
    "        # print('inside decorator.')\n",
    "        out, r_out = func(self, *args, **kwargs)\n",
    "        \n",
    "        # print(f\"r_out.shape: {r_out.shape}\")\n",
    "        \n",
    "        try:\n",
    "            text = kwargs.get(\"text\", None)\n",
    "            x_batch = kwargs.get(\"x_batch\", None)            \n",
    "            # convert text to x_batch\n",
    "            if not x_batch and text:\n",
    "                sample_word_vecs = [tokenize_and_vectorize_sample(sample, max_len=max_len) for sample in [text]]\n",
    "                word_list = [word for sample in sample_word_vecs for word, *_ in sample]\n",
    "                x_wordvec_list = [vec for sample in sample_word_vecs for *_, vec in sample]\n",
    "                x_batch = np.array([x_wordvec_list])\n",
    "                # x_batch = [pad_trunc(sample, max_len=max_len, embedding_dims=embedding_dims) for sample in x_batch]        \n",
    "                x_batch = to_device(Variable(torch.FloatTensor(x_batch)), device)\n",
    "            # print(f\"x_batch.shape:{x_batch.shape}\")\n",
    "            \n",
    "            ###### begin experimental\n",
    "            # calculate effect of each word without history\n",
    "            # print(f\"x_batch.shape={x_batch.shape}\")\n",
    "            r_singles_hns = [self.lstm(word.unsqueeze(0), None) for word in x_batch.squeeze(0).squeeze(0)]        \n",
    "            # print(f\"r_out.shape: {r_out.shape}\")        \n",
    "            dot_products = [torch.dot(r_singles.squeeze(0),r_out[:,-1, :].squeeze(0)).item() for r_singles,*_ in r_singles_hns]\n",
    "            # calculate cosine similarity of final snowball with each historyless word effect\n",
    "            cos = nn.CosineSimilarity(dim=0, eps=1e-6)        \n",
    "            cos_similarities = [cos(r_singles.squeeze(0),r_out[:,-1, :].squeeze(0)).item() for r_singles,*_ in r_singles_hns]\n",
    "\n",
    "            # combine position, word, scores for logging purpose\n",
    "            # word_idx  = [idx for idx,*_ in enumerate(word_list)]\n",
    "            word_scores = list(set((zip(word_list, dot_products, cos_similarities))))\n",
    "            word_scores = sorted(word_scores, key=lambda x:-x[-2])\n",
    "            \n",
    "            # plot top 10\n",
    "            plot_top_k_scores(word_scores,k=10,key=-2,  y_label=\"word_scores\")\n",
    "            word_scores = '\\n'.join([str(ws) for ws in word_scores])\n",
    "            print(text)\n",
    "            print(f\"word_scores: {(word_scores)}\")\n",
    "            \n",
    "        except Exception as err:\n",
    "            print(traceback.format_exc())\n",
    "        # print(f\"exiting wrapped\")\n",
    "        return out, r_out \n",
    "    return wrapped\n",
    "\n",
    "\n",
    "def exp_2(func):\n",
    "    \"\"\"\n",
    "    Goal: How much snowball at timestamp t is moved from the word at timestep t\n",
    "    Experiment: calculate force exerted by history on each word\n",
    "    assumption: supporting words will be displaced less, opposing words will be displaced more by the history. Displacement means distance between input word and output of a cell\n",
    "    findings: ?\n",
    "    \"\"\"\n",
    "    def wrapped(self, *args, **kwargs):\n",
    "        # print('inside decorator.')\n",
    "        out, r_out = func(self, *args, **kwargs)\n",
    "        \n",
    "\n",
    "        try:\n",
    "            text = kwargs.get(\"text\", None)\n",
    "            x_batch = kwargs.get(\"x_batch\", None)            \n",
    "            # convert text to x_batch\n",
    "            if not x_batch and text:\n",
    "                sample_word_vecs = [tokenize_and_vectorize_sample(sample, max_len=max_len) for sample in [text]]\n",
    "                word_list = [word for sample in sample_word_vecs for word, *_ in sample]\n",
    "                x_wordvec_list = [vec for sample in sample_word_vecs for *_, vec in sample]\n",
    "                x_batch = np.array([x_wordvec_list])\n",
    "                # x_batch = [pad_trunc(sample, max_len=max_len, embedding_dims=embedding_dims) for sample in x_batch]        \n",
    "                x_batch = to_device(Variable(torch.FloatTensor(x_batch)), device)\n",
    "                        \n",
    "            # calculate representation of each word without history\n",
    "            r_singles_hns = [self.lstm(word.unsqueeze(0), None) for word in x_batch.squeeze(0).squeeze(0)]        \n",
    "            \n",
    "            \n",
    "            dot_products = [torch.dot(r_singles.squeeze(0),r_out[:,i, :].squeeze(0)).item() for i, (r_singles,*_) in enumerate(r_singles_hns)]\n",
    "            \n",
    "            # calculate cosine similarity of snowball at timestamp t with each historyless word effect\n",
    "            cos = nn.CosineSimilarity(dim=0, eps=1e-6)        \n",
    "            cos_similarities = [cos(r_singles.squeeze(0),r_out[:,i, :].squeeze(0)).item() for i, (r_singles,*_) in enumerate(r_singles_hns)]\n",
    "\n",
    "            # combine position, word, scores for logging purpose\n",
    "            word_displacements = list(set((zip(word_list, dot_products, cos_similarities))))\n",
    "            word_displacements = sorted(word_displacements, key=lambda x:-x[-2])\n",
    "            \n",
    "            # plot top 10\n",
    "            plot_top_k_scores(word_displacements,k=10,key=-2, y_label=\"word_displacements\")\n",
    "            \n",
    "            word_displacements = '\\n'.join([str(wd) for wd in word_displacements])\n",
    "            print(text)\n",
    "            print(f\"word_displacements: {(word_displacements)}\")\n",
    "\n",
    "            # end experimental\n",
    "        except Exception as err:\n",
    "            print(traceback.format_exc())\n",
    "        \n",
    "        return out, r_out \n",
    "    return wrapped\n",
    "\n",
    "\n",
    "def exp_3(func):\n",
    "    \"\"\"\n",
    "    Goal: Calculate which words(enforcers) lead to maximum change in cell output/snowball\n",
    "    assumption: supporting words will be displaced less, opposing words will be displaced more by the history. Displacement means distance between input word and output of a cell\n",
    "    findings: ?  \n",
    "    \"\"\"\n",
    "    def wrapped(self, *args, **kwargs):\n",
    "        out, r_out = func(self, *args, **kwargs)\n",
    "        \n",
    "        try:\n",
    "            text = kwargs.get(\"text\", None)\n",
    "            x_batch = kwargs.get(\"x_batch\", None)            \n",
    "            # convert text to x_batch\n",
    "            if not x_batch and text:\n",
    "                sample_word_vecs = [tokenize_and_vectorize_sample(sample, max_len=max_len) for sample in [text]]\n",
    "                word_list = [word for sample in sample_word_vecs for word, *_ in sample]\n",
    "                x_wordvec_list = [vec for sample in sample_word_vecs for *_, vec in sample]\n",
    "                x_batch = np.array([x_wordvec_list])\n",
    "                # x_batch = [pad_trunc(sample, max_len=max_len, embedding_dims=embedding_dims) for sample in x_batch]        \n",
    "                x_batch = to_device(Variable(torch.FloatTensor(x_batch)), device)\n",
    "            \n",
    "            total_words = len(list(x_batch.squeeze(0).squeeze(0))\n",
    "                              \n",
    "            dot_products = [torch.dot(r_out[:,i, :].squeeze(0), r_out[:,i+1, :].squeeze(0)).item() for i in range(total_words-1)]\n",
    "            \n",
    "            # calculate cosine similarity between two adjacent cell outputs\n",
    "            cos = nn.CosineSimilarity(dim=0, eps=1e-6)        \n",
    "            cos_similarities = [cos(r_out[:,i, :].squeeze(0), r_out[:,i+1, :].squeeze(0)).item() for i in range(total_words-1)]\n",
    "\n",
    "            # combine position, word, scores for logging purpose\n",
    "            # word_idx  = [idx for idx,*_ in enumerate(word_list)]\n",
    "            history_displacements = list(set((zip(word_list[1:], dot_products, cos_similarities))))\n",
    "            history_displacements = sorted(history_displacements, key=lambda x:-x[-2])\n",
    "            \n",
    "            plot_top_k_scores(history_displacements,k=10,key=-2, y_label=\"history_displacements\")\n",
    "            \n",
    "            history_displacements = '\\n'.join([str(wd) for wd in history_displacements])\n",
    "            print(text)\n",
    "            print(f\"history_displacements: {(history_displacements)}\")\n",
    "\n",
    "            # end experimental\n",
    "        except Exception as err:\n",
    "            print(traceback.format_exc())\n",
    "        return out, r_out \n",
    "    return wrapped\n",
    "\n",
    "\n",
    "class ExperimentalLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dims=300, hidden_dims=100, num_layers=1, batch_first=True):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dims,\n",
    "            hidden_size=hidden_dims,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Linear(hidden_dims, 1)\n",
    "        \n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    \n",
    "    # use decorator for experimentation during inference \n",
    "    @exp_3\n",
    "    def forward(self, text:str=\"\", x_batch=None):\n",
    "        #1.  get words,vecs from text\n",
    "        \n",
    "        #TODO: avoid repetetion\n",
    "        #if True: #not x_batch and text:            \n",
    "        sample_word_vecs = [tokenize_and_vectorize_sample(sample, max_len=max_len) for sample in [text]]\n",
    "        word_list = [word for sample in sample_word_vecs for word, *_ in sample]\n",
    "        x_wordvec_list = [vec for sample in sample_word_vecs for *_, vec in sample]\n",
    "        \n",
    "        x_batch = np.array([x_wordvec_list])\n",
    "        # x_batch = [pad_trunc(sample, max_len=max_len, embedding_dims=embedding_dims) for sample in x_batch]        \n",
    "        x_batch = to_device(Variable(torch.FloatTensor(x_batch)),device)\n",
    "        \n",
    "        # print(f\"calling self.lstm\")\n",
    "        r_out, h_n = self.lstm(x_batch, None)\n",
    "        print(f\"r_out.shape:{r_out.shape}\")\n",
    "\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        out = self.activation(out)\n",
    "        return out, r_out #final outputs and intermediate histories(r_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "15381fad-21fd-4580-8843-1d901bbea32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims=300\n",
    "hidden_dims= 100\n",
    "num_layers=2\n",
    "batch_first=True\n",
    "max_len=400\n",
    "epochs = 5\n",
    "batch_size=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d878520f-99ab-4f9c-bc89-e98552ef5ecc",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb00130-28e1-41a6-be3f-915b50e32047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "\n",
    "lstm_model = LSTM(embedding_dims=300, hidden_dims=100, num_layers=num_layers, batch_first=True)\n",
    "lstm_model = to_device(lstm_model, device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters())\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = epochs  # Example value for epochs\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    lstm_model.train()     \n",
    "    loss_val = 0\n",
    "    for i, (x_batch, y_batch) in enumerate(generate_batch(x_train, y_train, batch_size=batch_size)):\n",
    "        # print(i)\n",
    "        x_batch = [tokenize_and_vectorize_sample(sample) for sample in x_batch]\n",
    "        x_batch = Variable(torch.FloatTensor(x_batch))\n",
    "        x_batch = to_device(x_batch, device)\n",
    "        # x_batch = x_batch.permute(0, 2, 1)\n",
    "        # print(x_batch.shape)\n",
    "        y_batch = to_device(Variable(torch.FloatTensor(np.array([y_batch]))), device)\n",
    "        y_batch = y_batch.reshape(batch_size,1)\n",
    "        outputs = lstm_model(x_batch)\n",
    "        # print(outputs)\n",
    "        # print(y_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        loss_val += loss.item()\n",
    "        optimizer.step()\n",
    "        # if i==20000:\n",
    "        #     break\n",
    "    print(f\"epoch({epoch}): total_loss={loss_val}\")\n",
    "    loss_val=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2079e2-e3d1-4c00-9cd6-cf6993aae4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pytorch model\n",
    "# saving embeddings\n",
    "model_path = f\"imdb_lstm_model_{max_len}_{embedding_dims}_{hidden_dims}_{num_layers}.pth\"\n",
    "torch.save(lstm_model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e6aac50f-1d68-4d84-bb03-c0f46a2f2004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file\n",
    "model_path = f\"imdb_lstm_model_{max_len}_{embedding_dims}_{hidden_dims}_{num_layers}.pth\"\n",
    "loaded_model = ExperimentalLSTM(embedding_dims=embedding_dims, hidden_dims=hidden_dims, num_layers=num_layers)  # Create an instance of your model\n",
    "loaded_model.load_state_dict(torch.load(model_path))  # Load the state dictionary\n",
    "loaded_model.eval()  # Set the model to evaluation mode\n",
    "loaded_model = to_device(loaded_model, device)\n",
    "# Now 'loaded_model' contains the model loaded from the saved file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8392c5-98e4-4c41-bc04-490008c0f05d",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57945cf-feb3-4c95-967e-caeda57fa535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the model and prepare input data (as shown in the previous responses)\n",
    "\n",
    "def evaluate(x_test, y_test, batch_size=1):\n",
    "    print(f\"len(x_test) == len(y_test): {len(x_test)} == {len(y_test)}\")\n",
    "    predictions = []\n",
    "    batches = int(len(x_test)/batch_size) +1\n",
    "    for i in tqdm(range(batches)):\n",
    "        x_batch = x_test[i:i+batch_size]\n",
    "        y_batch = y_test[i:i+batch_size]\n",
    "        \n",
    "        if not x_batch or not y_batch:\n",
    "            break\n",
    "        \n",
    "        x_batch = [tokenize_and_vectorize_sample(sample, max_len=max_len) for sample in x_batch]\n",
    "        \n",
    "        x_batch = Variable(torch.FloatTensor(x_batch))\n",
    "        x_batch = to_device(x_batch, device)\n",
    "        \n",
    "        \n",
    "        y_batch = to_device(Variable(torch.FloatTensor([y_batch])), device)\n",
    "        y_batch = y_batch.reshape(batch_size,1)\n",
    "        y_batch = to_device(y_batch, device)\n",
    "        # print(x_batch.shape)\n",
    "        # Perform inference on the test data\n",
    "        with torch.no_grad():\n",
    "            # Forward pass to get predictions\n",
    "            batch_predictions = loaded_model(x_batch)\n",
    "            # print(f\"batch_predictions: {batch_predictions}\")\n",
    "            # Assuming 'predictions' is the model's predictions (binary values)\n",
    "            # print(predictions)\n",
    "            # Convert predictions to binary values based on a threshold (e.g., 0.5 for binary classification)\n",
    "            threshold = 0.5\n",
    "            binary_predictions = (batch_predictions > threshold).float()\n",
    "            binary_predictions = [bp.squeeze(0).cpu() for bp in binary_predictions]\n",
    "            # print(f\"binary_predictions.squeeze(): {binary_predictions.squeeze()}\")\n",
    "            predictions.extend(binary_predictions)\n",
    "            \n",
    "            # print(f\"len(binary_predictions):{len(binary_predictions)}\")\n",
    "    \n",
    "    total = min(len(y_test), len(predictions))\n",
    "    # print(y_test[:total])\n",
    "    # print(predictions[:total])\n",
    "    accuracy = accuracy_score(y_test[:total], predictions[:total])\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "evaluate(x_test, y_test)\n",
    "# print(type(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62b1a71-1daa-4176-a91a-bf7fbcdc043b",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34293b0-447d-4af6-b8cb-a3c84fb60847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text):\n",
    "    sample_word_vecs = [tokenize_and_vectorize_sample(sample, max_len=max_len) for sample in [text]]\n",
    "    word_list = [word for sample in sample_word_vecs for word, *_ in sample]\n",
    "    x_wordvec_list = [vec for sample in sample_word_vecs for *_, vec in sample]\n",
    "    print(f\"len(x_wordvec_list): {len(x_wordvec_list)}\")\n",
    "    print(f\"type(x_wordvec_list[0]): {type(x_wordvec_list[0])}\")\n",
    "    print(word_list)\n",
    "    x_batch = np.array([x_wordvec_list])\n",
    "    # x_batch = [pad_trunc(sample, max_len=max_len, embedding_dims=embedding_dims) for sample in x_batch]        \n",
    "    x_batch = to_device(Variable(torch.FloatTensor(x_batch)),device)\n",
    "    # x_batch = x_batch.permute(0, 2, 1)\n",
    "    # print(x_batch.shape)\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        # Forward pass to get predictions\n",
    "        predictions = loaded_model(x_batch)\n",
    "        # If you're doing binary classification (as in your original Keras model)\n",
    "        # You may want to threshold the predictions to get the final classes\n",
    "        # Assuming a threshold of 0.5 for binary classification\n",
    "        # print(predictions)\n",
    "        threshold = 0.5\n",
    "        binary_predictions = (predictions > threshold).float()\n",
    "        return binary_predictions\n",
    "\n",
    "review = inference(\"\"\"Movie was good and it felt like a good movie\"\"\")\n",
    "\n",
    "print(review)\n",
    "\n",
    "# print(x_train[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ba8d37dd-6cab-4953-b318-d694addc8092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_id:26046\n",
      "actual:[1]\n",
      "inside forward\n",
      "r_out.shape:torch.Size([1, 125, 100])\n",
      "leaving forward\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tom had a wish to make film for a long time and he did. it is as if he has visualized a dirty and worn out notebook full of great little ideas he has been filling up and carrying around for 10 years. no grant character transformations, no Hollywood ingredients just life and a little bit of magic. the balance, in speed, in weirdness in comedy vs drama is perfectly weighed. this film takes you on a journey that is over before you realize how nice it was. the music is great and your eyes will be equally satisfied. the fact that this film is about nothing, merely a sequence of sketches of people that are mainly linked trough a party of one of the characters, makes it very pleasant and surprisingly entertaining, it is brilliant because it is empty. in between the lines it is happening. to see or not to see, that's the ?\n",
      "history_displacements: ('not', 8.78446102142334, 0.823113203048706)\n",
      "('entertaining', 7.553743362426758, 0.7668375968933105)\n",
      "('bit', 6.683976173400879, 0.8900999426841736)\n",
      "('it', 6.587815284729004, 0.7041637301445007)\n",
      "('see', 6.303319931030273, 0.792668342590332)\n",
      "('it', 6.1834187507629395, 0.7035356760025024)\n",
      "('surprisingly', 6.1592607498168945, 0.6821762323379517)\n",
      "('sequence', 6.1096601486206055, 0.9109455347061157)\n",
      "('merely', 6.0789337158203125, 0.7813055515289307)\n",
      "('be', 6.0525970458984375, 0.7680580615997314)\n",
      "('the', 5.966901779174805, 0.8018929362297058)\n",
      "('the', 5.567183971405029, 0.9245229959487915)\n",
      "('is', 5.53865909576416, 0.7078466415405273)\n",
      "('will', 5.080081939697266, 0.6083952784538269)\n",
      "('or', 4.966477394104004, 0.674403727054596)\n",
      "('is', 4.953939437866211, 0.7543628215789795)\n",
      "('the', 4.83433723449707, 0.8315543532371521)\n",
      "('it', 4.782282829284668, 0.7237696051597595)\n",
      "('that', 4.7608747482299805, 0.8163769245147705)\n",
      "('Hollywood', 4.704040050506592, 0.6053793430328369)\n",
      "('ingredients', 4.634081840515137, 0.7197091579437256)\n",
      "('because', 4.63050651550293, 0.4716280698776245)\n",
      "('brilliant', 4.603706359863281, 0.5718269944190979)\n",
      "('is', 4.588022708892822, 0.7370889782905579)\n",
      "('that', 4.230832576751709, 0.6319562196731567)\n",
      "('music', 4.185966491699219, 0.7246346473693848)\n",
      "('party', 4.021237373352051, 0.7431779503822327)\n",
      "('in', 3.996885299682617, 0.7140387892723083)\n",
      "('between', 3.966935873031616, 0.8561145067214966)\n",
      "('before', 3.9016900062561035, 0.8567491769790649)\n",
      "('eyes', 3.8444221019744873, 0.5936251878738403)\n",
      "('nice', 3.823178768157959, 0.6688610911369324)\n",
      "('he', 3.750101089477539, 0.6821765303611755)\n",
      "('worn', 3.721933603286743, 0.7018678188323975)\n",
      "('equally', 3.712956428527832, 0.7053889632225037)\n",
      "('lines', 3.631664752960205, 0.7522628307342529)\n",
      "('nothing', 3.6077113151550293, 0.674669623374939)\n",
      "('just', 3.5887441635131836, 0.8874392509460449)\n",
      "('pleasant', 3.5869956016540527, 0.5619354844093323)\n",
      "('over', 3.5492401123046875, 0.6960619688034058)\n",
      "('the', 3.4814252853393555, 0.8419444561004639)\n",
      "('ideas', 3.426231861114502, 0.619614839553833)\n",
      "('sketches', 3.414618968963623, 0.7088943123817444)\n",
      "('that', 3.3867974281311035, 0.9313336610794067)\n",
      "('mainly', 3.3829290866851807, 0.8058223724365234)\n",
      "('are', 3.321153402328491, 0.739635169506073)\n",
      "('is', 3.307213306427002, 0.5928892493247986)\n",
      "('see', 3.276829957962036, 0.681718111038208)\n",
      "('it', 3.271737575531006, 0.7470142841339111)\n",
      "('how', 3.254509449005127, 0.7493273019790649)\n",
      "('journey', 3.139561176300049, 0.5353603363037109)\n",
      "('grant', 3.0955281257629395, 0.5003381967544556)\n",
      "('one', 3.0784780979156494, 0.720770001411438)\n",
      "('that', 3.073605537414551, 0.6974565982818604)\n",
      "('great', 3.0354530811309814, 0.5757688879966736)\n",
      "('your', 3.0345876216888428, 0.5648090839385986)\n",
      "('notebook', 3.013840913772583, 0.7187557220458984)\n",
      "('linked', 2.9836490154266357, 0.6606251001358032)\n",
      "('fact', 2.95536470413208, 0.8451329469680786)\n",
      "('character', 2.8937149047851562, 0.6516282558441162)\n",
      "('is', 2.888751745223999, 0.705430805683136)\n",
      "('trough', 2.832770586013794, 0.5795197486877441)\n",
      "('been', 2.8290343284606934, 0.8344243764877319)\n",
      "('dirty', 2.661684989929199, 0.6810837984085083)\n",
      "('very', 2.642444610595703, 0.6596276760101318)\n",
      "('balance', 2.553234577178955, 0.6293793320655823)\n",
      "('on', 2.548454761505127, 0.5194505453109741)\n",
      "('full', 2.511080026626587, 0.7042228579521179)\n",
      "('people', 2.5107951164245605, 0.6131207942962646)\n",
      "('the', 2.5093953609466553, 0.6530125141143799)\n",
      "('up', 2.476649522781372, 0.8005654811859131)\n",
      "('this', 2.470010280609131, 0.8025817275047302)\n",
      "('filling', 2.4308836460113525, 0.6129688024520874)\n",
      "('out', 2.370905637741089, 0.5050698518753052)\n",
      "('has', 2.358184814453125, 0.6041517853736877)\n",
      "('he', 2.263218641281128, 0.7484354972839355)\n",
      "('realize', 2.221813201904297, 0.6071186661720276)\n",
      "('little', 2.18087100982666, 0.4540775418281555)\n",
      "('you', 2.0832576751708984, 0.5816622972488403)\n",
      "('make', 2.0026843547821045, 0.7382285594940186)\n",
      "('the', 1.9642988443374634, 0.7798601388931274)\n",
      "('film', 1.9429559707641602, 0.5419777035713196)\n",
      "('around', 1.8438634872436523, 0.8228790760040283)\n",
      "('has', 1.8419291973114014, 0.5736715197563171)\n",
      "('is', 1.75481116771698, 0.5170944929122925)\n",
      "('carrying', 1.6889047622680664, 0.6852972507476807)\n",
      "('visualized', 1.6398589611053467, 0.56023770570755)\n",
      "('little', 1.6006547212600708, 0.5873748660087585)\n",
      "('great', 1.5650755167007446, 0.4709085524082184)\n",
      "('no', 1.514888882637024, 0.28516337275505066)\n",
      "('in', 1.4268805980682373, 0.5489602088928223)\n",
      "('about', 1.4204963445663452, 0.5067813992500305)\n",
      "('it', 1.377799391746521, 0.5916724801063538)\n",
      "('it', 1.3675447702407837, 0.37542688846588135)\n",
      "('you', 1.3658167123794556, 0.41593968868255615)\n",
      "('life', 1.3488329648971558, 0.6801260113716125)\n",
      "('he', 1.3456016778945923, 0.7407020926475525)\n",
      "('film', 1.3338415622711182, 0.5125948786735535)\n",
      "('characters', 1.2791193723678589, 0.5013700723648071)\n",
      "('for', 1.2700780630111694, 0.908781886100769)\n",
      "('if', 1.1947646141052246, 0.5412663817405701)\n",
      "('film', 1.1661146879196167, 0.444560706615448)\n",
      "('wish', 1.1589664220809937, 0.451960027217865)\n",
      "('speed', 1.1484826803207397, 0.4699164628982544)\n",
      "('had', 1.1234833002090454, 0.4440516531467438)\n",
      "('makes', 1.1134241819381714, 0.32189854979515076)\n",
      "('this', 1.1009209156036377, 0.5479300618171692)\n",
      "('transformations', 1.0945899486541748, 0.31442856788635254)\n",
      "('is', 1.0904605388641357, 0.6075439453125)\n",
      "('in', 1.0689456462860107, 0.589348554611206)\n",
      "('as', 0.9421051144599915, 0.5107851624488831)\n",
      "('for', 0.9269953370094299, 0.5719706416130066)\n",
      "('comedy', 0.8907278180122375, 0.5488952398300171)\n",
      "('takes', 0.8744357228279114, 0.2778412103652954)\n",
      "('no', 0.788872241973877, 0.21036958694458008)\n",
      "('in', 0.771447479724884, 0.489021360874176)\n",
      "('drama', 0.711797833442688, 0.375335156917572)\n",
      "('vs', 0.6720477342605591, 0.3410651385784149)\n",
      "('years.', 0.6290346384048462, 0.4531092047691345)\n",
      "('weirdness', 0.5541765093803406, 0.32415133714675903)\n",
      "('long', 0.40189623832702637, 0.5816773772239685)\n",
      "('time', 0.3806982636451721, 0.541893482208252)\n",
      "('is', 0.3577771782875061, 0.3249979615211487)\n",
      "('perfectly', 0.2780308127403259, 0.2540351152420044)\n",
      "tensor([[0.9988]], device='cuda:0')\n",
      "predicted: tensor([[1.]], device='cuda:0')\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "from termcolor import colored\n",
    "import re\n",
    "def experimental_inference(text):\n",
    "    # x_batch = x_batch.permute(0, 2, 1)\n",
    "    # print(x_batch.shape)\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        # Forward pass to get predictions\n",
    "        predictions, *_ = loaded_model(text=text, x_batch=None)\n",
    "        # If you're doing binary classification (as in your original Keras model)\n",
    "        # You may want to threshold the predictions to get the final classes\n",
    "        # Assuming a threshold of 0.5 for binary classification\n",
    "        print(predictions)\n",
    "        threshold = 0.5\n",
    "        binary_predictions = (predictions > threshold).float()\n",
    "        return binary_predictions\n",
    "\n",
    "# print(x_test[1])\n",
    "texts = [\n",
    "    \"movie was terrible and acting was great.\",\n",
    "    \"acting was great and movie was terrible.\"\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "sample_id = randrange(len(x_train))\n",
    "print(f\"sample_id:{sample_id}\")\n",
    "\n",
    "for text in x_train[sample_id:sample_id+1]:\n",
    "    try:\n",
    "        # print(f\"{text}\")\n",
    "        print(f\"actual:{y_train[sample_id:sample_id+1]}\")        \n",
    "        text = re.sub('<[^<]+?>', '', text)\n",
    "        review = experimental_inference(text)        \n",
    "        print(f\"predicted: {review}\")        \n",
    "        print(\"=\"*100)\n",
    "    except Exception as err:\n",
    "        pass\n",
    "    \n",
    "# \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
