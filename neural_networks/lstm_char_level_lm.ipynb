{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdfee76e-5efe-4149-87ef-e2eeb61c0dbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T06:53:37.286976397Z",
     "start_time": "2023-10-11T06:53:36.122875087Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2964fd89cd730f2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9d9aae6-d960-4837-b43d-00c328640f5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T06:53:25.814928925Z",
     "start_time": "2023-10-11T06:53:25.747616776Z"
    }
   },
   "outputs": [],
   "source": [
    "# create an index to character mapping\n",
    "import string\n",
    "\n",
    "idx2char = [\" \"] + list(string.ascii_lowercase)\n",
    "char2idx = {ch: idx for idx, ch in enumerate(idx2char)}\n",
    "\n",
    "\n",
    "def get_max_seq_length(filename=\"data/names/English.txt\"):\n",
    "    max_len = 0\n",
    "    with open(filename, \"r\") as fp:\n",
    "        for line in fp.readlines():\n",
    "            try:\n",
    "                line = line.lower().strip()\n",
    "                max_len = max(len(line), max_len)\n",
    "            except Exception as err:\n",
    "                continue\n",
    "    return max_len\n",
    "\n",
    "\n",
    "def prepare_rnn_data(filename=\"data/names/English.txt\", padding=True) -> list:\n",
    "    x_data = []\n",
    "\n",
    "    max_len = get_max_seq_length(filename)\n",
    "\n",
    "    with open(filename, \"r\") as fp:\n",
    "        for line in fp.readlines():\n",
    "            try:\n",
    "                line = line.lower().strip()\n",
    "                if padding:\n",
    "                    # pad strings with extra spaces to make them of equal length\n",
    "                    max_len = max(len(line), max_len)\n",
    "                    line += \" \" * max(max_len - len(line), 0)\n",
    "                    assert len(line) == max_len\n",
    "                line_idx = [char2idx[ch] for ch in line]\n",
    "                x_data.append(line_idx)\n",
    "            except Exception as err:\n",
    "                # print(line)\n",
    "                continue\n",
    "    return x_data\n",
    "\n",
    "\n",
    "def idx2onehot(idx: int) -> list:\n",
    "    \"\"\"\n",
    "    return one hot encoding for a given index\n",
    "    :param idx: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    one_hot = [0] * 27\n",
    "    one_hot[idx] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def convert_data_to_onehot(x_data: list) -> list:\n",
    "    \"\"\"\n",
    "    convert the indices to one hot encoding\n",
    "    :param x_data: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    x_one_hot = []\n",
    "    for lst in x_data:\n",
    "        one_hots = [idx2onehot(idx) for idx in lst]\n",
    "        x_one_hot.append(one_hots)\n",
    "    return x_one_hot\n",
    "\n",
    "\n",
    "rnn_data = prepare_rnn_data(filename=\"data/names/English.txt\")\n",
    "# leave out the last character as we will be predicting the next character\n",
    "x_data = [list[:-1] for list in rnn_data]\n",
    "\n",
    "x_one_hot = convert_data_to_onehot(x_data)\n",
    "\n",
    "# leave first character as we will be predicting the next character\n",
    "y_data = [lst[1:] for lst in rnn_data]\n",
    "\n",
    "assert len(x_one_hot) == len(y_data)\n",
    "# x_one_hot = torch.LongTensor(x_one_hot)\n",
    "y_data = torch.LongTensor(y_data)\n",
    "# print(y_data[0])\n",
    "\n",
    "inputs = torch.Tensor(x_one_hot)\n",
    "labels = torch.LongTensor(y_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d944ee57-ce06-4dd6-8f04-377d8455ebd7",
   "metadata": {},
   "source": [
    "# hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38b93347-dd32-4240-b80d-5cb7aceb6df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "seq_len = get_max_seq_length(filename=\"data/names/English.txt\")\n",
    "\n",
    "input_size = 27  # one-hot size\n",
    "batch_size = 5  # one sentence per batch\n",
    "num_layers = 1  # one-layer rnn\n",
    "num_classes = 27  # predicting 5 distinct character\n",
    "hidden_size = 128  # output from the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cf684a-c59a-4557-901a-67b7472699e2",
   "metadata": {},
   "source": [
    "# model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b1917b3-0841-4e1e-8a29-468527a58f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model will be a RNN followed by a linear layer,\n",
    "    i.e. a fully-connected layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seq_len, num_classes, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_size = hidden_size\n",
    "        # batch_first = true means forward will take input of shape (batch_size, seq_len, input_size)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # assuming batch_first = True for RNN cells\n",
    "        # print(f\"x.shape: {x.shape}\")\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        input_size = x.size(2)\n",
    "        # print(f\"batch_size: {batch_size}\")\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "        # print(f\"hidden.shape: {hidden.shape}\")\n",
    "        # print(f\"before x.shape: {x.shape}\")\n",
    "        # x = x.view(batch_size, seq_len, input_size)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x, hidden)\n",
    "        assert lstm_out.shape == (batch_size, seq_len, hidden_size)\n",
    "        assert h_n.shape == (1, batch_size, hidden_size)\n",
    "        # print(f\"rnn_out:{rnn_out}\")\n",
    "        # print(f\"rnn_out.shape: {rnn_out.shape}\")\n",
    "        # print(f\"rnn_out.shape: {rnn_out.view(-1, hidden_size).shape}\")\n",
    "        # rnn_out.contiguous().view(-1, hidden_size)\n",
    "        linear_out = self.linear(lstm_out.contiguous().view(-1, hidden_size))\n",
    "        assert linear_out.shape == (batch_size * seq_len, num_classes)\n",
    "        return linear_out\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize hidden cell states, assuming\n",
    "        batch_first = True for RNN cells\n",
    "        \"\"\"\n",
    "        return #torch.zeros(1, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42663e79-240d-44ea-b016-1bee54993432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network architecture:\n",
      " LSTM(\n",
      "  (lstm): LSTM(27, 128, batch_first=True)\n",
      "  (linear): Linear(in_features=128, out_features=27, bias=True)\n",
      ")\n",
      "epoch: 100, loss: 0.841\n",
      "epoch: 200, loss: 0.549\n",
      "epoch: 300, loss: 0.439\n",
      "epoch: 400, loss: 0.412\n",
      "epoch: 500, loss: 0.404\n"
     ]
    }
   ],
   "source": [
    "# Set loss, optimizer and the LSTM model\n",
    "torch.manual_seed(777)\n",
    "lstm_model = LSTM(seq_len, num_classes, input_size, hidden_size, num_layers=num_layers)\n",
    "print('network architecture:\\n', lstm_model)\n",
    "\n",
    "# train the model\n",
    "num_epochs = 500\n",
    "\n",
    "# This criterion computes the cross entropy loss between input logits and target.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.025)\n",
    "# print(f\"inputs.shape: {inputs.shape}\")\n",
    "# print(f\"labels.shape: {labels.shape}\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    optimizer.zero_grad()\n",
    "    # for idx in np.arange(0,len(inputs),step=batch_size):\n",
    "    # print(f\"ip.shape: {ip.shape}\")\n",
    "    # ip = inputs[idx:idx+batch_size,:,:]\n",
    "    # print(f\"inputs.shape: {inputs.shape}\")\n",
    "    # for idx, ip in enumerate(inputs):\n",
    "    # print(f\"inputs.shape: {inputs.shape}\")\n",
    "    outputs = lstm_model(inputs)\n",
    "    # assert outputs.shape == labels.view(-1).shape\n",
    "    # print(f\"outputs.shape: {outputs.shape}\")\n",
    "    # print(f\"labels.view(-1).shape: {labels.view(-1).shape}\")\n",
    "    assert outputs.shape[0] == labels.view(-1).shape[0]\n",
    "    loss = criterion(outputs, labels.view(-1).long())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # check the current predicted string\n",
    "    # max gives the maximum value and its\n",
    "    # corresponding index, we will only\n",
    "    # be needing the index\n",
    "    _, idx = outputs.max(dim=1)\n",
    "    idx = idx.detach().numpy()\n",
    "    result_str = [idx2char[c] for c in idx]\n",
    "    if epoch % 100 == 0:\n",
    "        print('epoch: {}, loss: {:1.3f}'.format(epoch, loss.item()))\n",
    "    # print(f\"type(result_str): {type(result_str)}\")\n",
    "    # print(f\"len(result_str): {len(result_str)}\")\n",
    "    # print('Predicted string: ', ''.join(result_str))\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a8d27ee-e3ed-41e7-8fd8-2621034df0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pytorch model\n",
    "# saving embeddings\n",
    "model_path = f\"lm_lstm_model_{seq_len}_{num_classes}_{input_size}_{hidden_size}.pth\"\n",
    "torch.save(lstm_model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3822a7a-10fb-4cd9-b280-21be54924790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model from the file\n",
    "model_path = f\"lm_lstm_model_{seq_len}_{num_classes}_{input_size}_{hidden_size}.pth\"\n",
    "loaded_model = LSTM(seq_len, num_classes, input_size, hidden_size,\n",
    "                   num_layers=num_layers)  # Create an instance of your model\n",
    "loaded_model.load_state_dict(torch.load(model_path))  # Load the state dictionary\n",
    "# loaded_model.eval()  # Set the model to evaluation mode\n",
    "# loaded_model = to_device(loaded_model, device)\n",
    "# Now 'loaded_model' contains the model loaded from the saved file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b25bfec0-2e4a-4d82-be7d-b667161cda7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in the model and character as arguments and returns the next character prediction and hidden state\n",
    "def predict(model, character):\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    character = [char2idx[c] for c in character]\n",
    "    character = [idx2onehot(c) for c in character]\n",
    "    character = torch.Tensor([character])\n",
    "    # character.to(device)\n",
    "    # print(f\"character.shape: {character.shape}\")\n",
    "    out = model(character)\n",
    "\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    # Taking the class with the highest probability score from the output\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "\n",
    "    return idx2char[char_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4532cc82-4801-48a3-99b6-a9b1036b0680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the desired output length and input characters as arguments, returning the produced sentence\n",
    "def sample(model, out_len, start='ab'):\n",
    "    model.eval()  # eval mode\n",
    "    start = start.lower()\n",
    "    max_len = 13\n",
    "    # start = start+\" \" * max(max_len - len(start), 0)\n",
    "    # First off, run through the starting characters\n",
    "    chars = [ch for ch in start]\n",
    "    # print(f\"chars: {chars}\")\n",
    "    size = out_len - len(chars)\n",
    "    # print(f\"size:{size}\")\n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for ii in range(size):\n",
    "        char, *h = predict(model, chars)\n",
    "        # print(f\"char: {char}\")\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85cbfd17-e6bc-4ead-85ba-5d35395b1cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zaoui   '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(loaded_model, 8, 'z')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
