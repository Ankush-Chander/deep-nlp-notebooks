{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:14:38.054774748Z",
     "start_time": "2023-08-30T07:14:37.255421179Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "device = get_default_device()\n",
    "print(device)\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "al_regex = re.compile(r\"[^a-zA-Z]\")\n",
    "\n",
    "\n",
    "class MBCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = \"data/mahabharat_gutenberg_lemmatized_sents.txt\"\n",
    "        with open(corpus_path) as fp:\n",
    "            for line in fp.readlines():\n",
    "                tokens = line.split()\n",
    "                tokens = [al_regex.sub('', token) for token in tokens]\n",
    "                yield tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary management\n",
    "#1. prepare word-idx dictionary, reverse dictionary\n",
    "#2. proabability table for negative sampling\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "mb_sents = MBCorpus()\n",
    "word_freq = defaultdict(int)\n",
    "for sentence in mb_sents:\n",
    "    for token in sentence:\n",
    "            word_freq[token]+=1\n",
    "\n",
    "# print(f\"word_freq: {word_freq}\")\n",
    "vocabulary = sorted(word_freq.keys())            \n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "\n",
    "\n",
    "total_count = sum([count for count in word_freq.values()])\n",
    "word_probabilities = [float(word_freq[idx2word[idx]]/total_count) for idx in range(vocabulary_size)]\n",
    "print(f\"vocabulary_size:{vocabulary_size}\")\n",
    "# print(sum(word_probabilities))\n",
    "\n",
    "# TODO 0: \n",
    "\"\"\"\n",
    "calculate TF_IDF matrix by looping from the corpus here. This matrix will be the lookup matric that takes you from \n",
    "word index to input vector\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SkipGramBatchModel(torch.nn.Module):\n",
    "    \"\"\" Center word as input, context words as target.\n",
    "        Objective is to maximize the score of map from input to target.\n",
    "    \"\"\"\n",
    "    def __init__(self, device, vocabulary_size, embedding_dim, neg_num=5, word_count=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        super(SkipGramBatchModel, self).__init__()\n",
    "        if word_count is None:\n",
    "            word_count = []\n",
    "        self.device = device\n",
    "        self.neg_num = neg_num\n",
    "        # TODO1: \n",
    "        \"\"\"\n",
    "        Current setup uses embedding layer, which takes as input vector indices and return vectors from embedding matrix \n",
    "        This works well when you start with one-hot vectors.\n",
    "        See first line in forward function. that takes centers and returns corresponding word vectors.\n",
    "        \n",
    "        Since requirement is to start with TF-IDF vectors of the words. Ypu would no longer have one-hot vectors, so Embedding layer wont work.\n",
    "        Replace target_embeddings(embedding layer) with Linear layer of same dimensions.\n",
    "        Refer: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "                \n",
    "        \"\"\"\n",
    "        self.target_embeddings = torch.nn.Embedding(vocabulary_size, embedding_dim)\n",
    "        self.context_embeddings = torch.nn.Embedding(vocabulary_size, embedding_dim)\n",
    "        initrange = 0.5 / embedding_dim\n",
    "        self.target_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.context_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        # if self.neg_num > 0:\n",
    "        #     self.table = create_sample_table(word_count)\n",
    "\n",
    "    def forward(self, centers, contexts):\n",
    "        # print(f\"batch_size: {batch_size}\")\n",
    "        # print(f\"self.target_embeddings.shape: {self.target_embeddings.shape}\")\n",
    "        \n",
    "        # TODO2: \n",
    "        \"\"\"\n",
    "        instead of passing just the target indices, pass actual TF_IDF vector here\n",
    "        \n",
    "        Forward pass will look like\n",
    "        u_embed = self.linear(batch of TF-IDF input_vectors)\n",
    "        \"\"\"\n",
    "        u_embeds = self.target_embeddings(centers)  \n",
    "        # print(f\"u_embeds.shape: {u_embeds.shape}\")        \n",
    "        v_embeds = self.context_embeddings(contexts)\n",
    "        # print(f\"v_embeds.shape: {v_embeds.shape}\")\n",
    "        # convert context embeddings from row vectors to column vectors via transpose\n",
    "        scores = torch.bmm(u_embeds, v_embeds.transpose(1,2)).squeeze()        \n",
    "        return scores \n",
    "\n",
    "    def get_embeddings(self):\n",
    "        return self.target_embeddings.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "from tqdm import tqdm\n",
    "embedding_dim = 100\n",
    "num_epochs = 5\n",
    "learning_rate = 0.025\n",
    "window_size = 5\n",
    "neg_num = 5\n",
    "word_count = None\n",
    "batch_size=128\n",
    "\n",
    "\n",
    "vocabulary_tensor = torch.arange(len(vocabulary)).to(device)  # Assuming vocabulary is a range or similar\n",
    "word_probabilities_tensor = torch.Tensor(word_probabilities).to(device)\n",
    "\n",
    "\n",
    "\n",
    "def generate_batch(data, batch_size=128, neg_num=4):\n",
    "    \"\"\"\n",
    "    generates batch_size number of  targets, batch_size * (1 positive + neg_num negative) contexts \n",
    "    \"\"\"\n",
    "    # centers, contexts = [], []\n",
    "    with tqdm(total=305796, position=0, leave=True) as pbar:\n",
    "        centers, contexts=[], []\n",
    "        for sent_idx, sentence in enumerate(data):\n",
    "            sentence = [word2idx[word] for word in sentence]\n",
    "\n",
    "            if sent_idx!=0 and sent_idx%100000==0:            \n",
    "                print(f\"processing {sent_idx}th sentence\")\n",
    "                # break\n",
    "\n",
    "            for target_position, target_idx in enumerate(sentence):                \n",
    "                context_indices = [sentence[idx] for idx in range(max(0, target_position - window_size), min(target_position + window_size + 1, len(sentence))) if idx != target_position]\n",
    "\n",
    "\n",
    "                if not context_indices:\n",
    "                    continue            \n",
    "                #print(f\"target_idx:{target_idx}, context_indices:{context_indices}\")\n",
    "\n",
    "                for context_idx in context_indices:\n",
    "                    centers.append([target_idx])\n",
    "                    p_contexts = [context_idx]\n",
    "                    # n_contexts = np.random.choice(len(vocabulary), neg_num, p=word_probabilities).tolist()\n",
    "                    n_contexts = torch.multinomial(word_probabilities_tensor, neg_num, replacement=True).cpu().tolist()\n",
    "\n",
    "                    # print(f\"{len(p_contexts)}+{len(n_contexts)} = {len(p_contexts + n_contexts)}\")\n",
    "                    contexts.append(p_contexts + n_contexts)\n",
    "                    \n",
    "                    # add ne\n",
    "                    if len(centers) == batch_size:\n",
    "                        yield torch.LongTensor(centers).to(device), torch.LongTensor(contexts).to(device)\n",
    "                        centers, contexts = [], [] \n",
    "            # break\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "def train_batch(clip=1.0, batch_size=128, neg_num=4):\n",
    "    batch_model = SkipGramBatchModel(device=device, vocabulary_size=vocabulary_size, embedding_dim=embedding_dim, neg_num=neg_num)\n",
    "    batch_model.to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        batch_model.parameters(), lr=learning_rate)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for i in tqdm(range(num_epochs)):\n",
    "        loss_val= 0\n",
    "        for j, (centers, contexts) in enumerate(generate_batch(mb_sents, batch_size=batch_size, neg_num=neg_num)):\n",
    "            y_pred = batch_model(centers, contexts)\n",
    "            y_true = [[1] + [0]*neg_num]*batch_size\n",
    "            y_true = torch.FloatTensor(y_true).to(device)\n",
    "            \n",
    "            loss = loss_fn(y_pred, y_true)\n",
    "            loss_val += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(batch_model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            # Print loss value at certain step\n",
    "            loss_val += loss.item()\n",
    "            \n",
    "        print(f\"Total loss at step {i}: {loss_val}\")\n",
    "        loss_val = 0\n",
    "    return batch_model.get_embeddings()\n",
    "\n",
    "# x = train()\n",
    "x = train_batch(batch_size=128)\n",
    "\n",
    "# %lprun -f generate_batch list(generate_batch(mb_sents,batch_size=2))\n",
    "# %lprun -f train_batch train_batch(batch_size=2)\n",
    "\n",
    "# todo: fix average loss\n",
    "# think batch variant: done\n",
    "# profile batch varient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving embeddings\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def save_embeddings(filename=\"embeddings.bin\", embeddings=None, dictionary=None):\n",
    "    \"\"\"Embeddings and reverse dictionary serialization and dump to a file.\"\"\"\n",
    "    data = {\n",
    "        'emb': embeddings,\n",
    "        'dict': dictionary\n",
    "    }\n",
    "    file = open(filename, 'wb')\n",
    "    print(\"Saving embeddings to file:\", filename)\n",
    "    pickle.dump(data, file)\n",
    "\n",
    "save_embeddings(filename=\"batchwise_embeddings.bin\", embeddings=x, dictionary=word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "class Word2Vec(object):\n",
    "    \"\"\"Inference interface of Word2Vec embeddings\n",
    "        Before inference the embdedding result of a word, data need to be initialized\n",
    "        by calling method from_file or from_object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.embeddings = None\n",
    "        self.dictionary = None\n",
    "        self.reverse_dictionary = None\n",
    "\n",
    "    def from_file(self, filename):\n",
    "        file = open(filename, 'rb')\n",
    "        data = pickle.load(file)\n",
    "        self.embeddings = data['emb']\n",
    "        self.dictionary = data['dict']\n",
    "        self.reverse_dictionary = {v:k for k,v in self.dictionary.items()}\n",
    "\n",
    "    def from_object(self, embeddings, dictionary):\n",
    "        self.embeddings = embeddings\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "    def inference(self, word):\n",
    "        assert self.embeddings is not None and self.dictionary is not None, \\\n",
    "            'Embeddings not initialized, use from_file or from_object to load data.'\n",
    "        word_idx = self.dictionary.get(word)\n",
    "        # Unknown word returns UNK's word_idx\n",
    "        if word_idx is None:\n",
    "            word_idx = 0\n",
    "        return self.embeddings[word_idx]\n",
    "\n",
    "    def similarity(self, word1, word2):\n",
    "        v1 = self.inference(word1)\n",
    "        v1 = v1.cpu().numpy()\n",
    "        v2 = self.inference(word2)\n",
    "        v2 = v2.cpu().numpy()\n",
    "        # perform cosine similarity using torch\n",
    "        return np.dot(v1, v2)  # / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "    def most_similar(self, word, topk=10):\n",
    "        assert self.embeddings is not None and self.dictionary is not None, \\\n",
    "            'Embeddings not initialized, use from_file or from_object to load data.'\n",
    "        word_idx = self.dictionary.get(word)\n",
    "        # Unknown word returns UNK's word_idx\n",
    "        if word_idx is None:\n",
    "            word_idx = 0\n",
    "        word_emb = self.embeddings[word_idx].cpu().numpy()\n",
    "        # word_emb = word_emb / np.linalg.norm(word_emb)\n",
    "        similarity = np.dot(self.embeddings.cpu().numpy(), word_emb)\n",
    "        sorted_idx = np.argsort(similarity)[::-1]\n",
    "        return [(self.reverse_dictionary.get(idx, \"\"), similarity[idx]) for idx in sorted_idx[:topk]]\n",
    "\n",
    "    def analogy(self, word1, word2, word3, topk=10):\n",
    "        assert self.embeddings is not None and self.dictionary is not None, \\\n",
    "            'Embeddings not initialized, use from_file or from_object to load data.'\n",
    "        word1_idx = self.dictionary.get(word1, 0)\n",
    "        word2_idx = self.dictionary.get(word2, 0)\n",
    "        word3_idx = self.dictionary.get(word3, 0)\n",
    "        # Unknown word returns UNK's word_idx\n",
    "\n",
    "        word1_emb = self.embeddings[word1_idx].cpu().numpy()\n",
    "        word2_emb = self.embeddings[word2_idx].cpu().numpy()\n",
    "        word3_emb = self.embeddings[word3_idx].cpu().numpy()\n",
    "        word4_emb = word2_emb - word1_emb + word3_emb\n",
    "        similarity = np.dot(self.embeddings.cpu().numpy(), word4_emb)\n",
    "        sorted_idx = np.argsort(similarity)[::-1]\n",
    "        return [(self.reverse_dictionary.get(idx, \"\"), similarity[idx]) for idx in sorted_idx[:topk]]\n",
    "\n",
    "    \n",
    "wv = Word2Vec()\n",
    "wv.from_file(\"batchwise_embeddings.bin\")\n",
    "for word in [\"Arjuna\", \"Drona\",\"Bhishma\", \"Krishna\", \"mace\", \"Karna\", \"Pandu\", \"Kunti\", \"Yudhishthira\"]:\n",
    "    print(f\"{word}: {wv.most_similar(word)}\")\n",
    "# wv.most_similar(\"Arjuna\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
