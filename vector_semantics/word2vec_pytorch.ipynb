{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:14:38.054774748Z",
     "start_time": "2023-08-30T07:14:37.255421179Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "device = get_default_device()\n",
    "print(device)\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "al_regex = re.compile(r\"[^a-zA-Z]\")\n",
    "\n",
    "\n",
    "class MBCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = \"data/mahabharat_gutenberg_lemmatized_sents.txt\"\n",
    "        with open(corpus_path) as fp:\n",
    "            for line in fp.readlines():\n",
    "                tokens = line.split()\n",
    "                tokens = [al_regex.sub('', token) for token in tokens]\n",
    "                yield tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mb_sents = MBCorpus()\n",
    "vocabulary = []\n",
    "for sentence in mb_sents:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "vocabulary = sorted(vocabulary)            \n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(f\"vocabulary_size:{vocabulary_size}\")\n",
    "\n",
    "# get all sentences into a tensor    \n",
    "s_time = time.time()\n",
    "VECTORIZED_SENTENCES = [torch.tensor([word2idx[word] for word in sentence], dtype=torch.long).cuda() for sentence in mb_sents]\n",
    "print(f\"total sentences: {len(VECTORIZED_SENTENCES)}\")\n",
    "print(f\"loading of sentence tensor took: {time.time()-s_time} secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timer_func(func):\n",
    "    # This function shows the execution time of \n",
    "    # the function object passed\n",
    "    def wrap_func(*args, **kwargs):\n",
    "        t1 = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        t2 = time.time()\n",
    "        print(f'Function {func.__name__!r} executed in {(t2-t1):.4f}s')\n",
    "        return result\n",
    "    return wrap_func\n",
    "  \n",
    "# @timer_func\n",
    "def get_input_layer(word_idx):    \n",
    "    x = torch.zeros(vocabulary_size).type(torch.FloatTensor)\n",
    "    x[word_idx] = 1.0\n",
    "    \n",
    "    return to_device(x, device)\n",
    "        \n",
    "    \n",
    "get_input_layer(10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:14:29.485418854Z",
     "start_time": "2023-08-30T07:14:29.479921397Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "embedding_dims = 100\n",
    "# define target matrix\n",
    "W1 = Variable(to_device(torch.randn(embedding_dims, vocabulary_size), device), requires_grad=True)\n",
    "\n",
    "# define context martix\n",
    "W2 = Variable(to_device(torch.randn(vocabulary_size, embedding_dims), device), requires_grad=True)\n",
    "\n",
    "num_epochs = 101\n",
    "learning_rate = 0.01\n",
    "window_size = 2\n",
    "\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    s_time = time.time()\n",
    "    loss_val = 0\n",
    "    # for each sentence\n",
    "    \n",
    "    s_time = time.time()\n",
    "    np.random.shuffle(VECTORIZED_SENTENCES)\n",
    "    print(f\"shuffling of sentence tensor took: {time.time()-s_time} secs\")\n",
    "    \n",
    "    for sent_idx, sentence in enumerate(VECTORIZED_SENTENCES):\n",
    "        \n",
    "        if sent_idx!=0 and sent_idx%100000==0:            \n",
    "            print(f\"processing {sent_idx}th sentence\")\n",
    "            # break\n",
    "            \n",
    "        for target_idx in sentence:\n",
    "            context_indices = [sentence[idx] for idx in range(max(0, target_idx - window_size), min(target_idx + window_size + 1, len(sentence))) if idx != target_idx]\n",
    "                        \n",
    "            \n",
    "            if not context_indices:\n",
    "                continue            \n",
    "            #print(f\"target_idx:{target_idx}, context_indices:{context_indices}\")\n",
    "            \n",
    "            for context_idx in context_indices:                               \n",
    "                \n",
    "                #1.forward pass\n",
    "                x = Variable(get_input_layer(target_idx))\n",
    "                y_true = Variable(to_device(torch.tensor([context_idx]).long(), device))\n",
    "                \n",
    "                # print(f\"x:{x}\")\n",
    "                # print(f\"y_true:{y_true}\")                \n",
    "                \n",
    "                # map one hot vector with it\"s correspopnding embedding\n",
    "                z1 = torch.matmul(W1, x)\n",
    "                \n",
    "                # generate score vector by dot product with other context vectors\n",
    "                z2_scores = torch.matmul(W2, z1)\n",
    "                \n",
    "                log_softmax = F.log_softmax(z2_scores, dim=0)\n",
    "                # print(f\"log_softmax.shape: {log_softmax.shape}\")\n",
    "                # print(f\"log_softmax.view(1,-1).shape: {log_softmax.view(1,-1).shape}\")                \n",
    "                \n",
    "                # loss calculation\n",
    "                loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "                loss_val += loss.item()\n",
    "                loss.backward()\n",
    "                \n",
    "                # optimization\n",
    "                W1.data -= learning_rate * W1.grad.data\n",
    "                W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "                W1.grad.data.zero_()\n",
    "                W2.grad.data.zero_()        \n",
    "    print(f'Loss at epo {epo}: {loss_val}')\n",
    "    print(f\"epoch#{epo} took {time.time()-s_time} secs\")\n",
    "\n",
    "# save weights \n",
    "torch.save(W1, \"W1_sg_simple.pt\")\n",
    "torch.save(W2, \"W2_sg_simple.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skipgram with negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of words for negative sampling\n",
    "from collections import Counter\n",
    "s_time = time.time()\n",
    "word_counts = Counter([word for sentence in VECTORIZED_SENTENCES for word in sentence])\n",
    "word_freqs = np.array([word_counts[word] for word in word2idx.keys()], dtype=np.float32)\n",
    "word_probs = word_freqs / word_freqs.sum()\n",
    "neg_sampling_candidates = list(word2idx.values())\n",
    "print(neg_sampling_candidates[:5])\n",
    "print(f\"negative sampling candidates generation took: {time.time() - s_time} secs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipgram with negative sampling\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "embedding_dims = 100\n",
    "num_epochs = 101\n",
    "learning_rate = 0.01\n",
    "window_size = 2\n",
    "negative_samples = 4\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "# initialize target matrix\n",
    "W1 = Variable(to_device(torch.randn(embedding_dims, vocabulary_size), device), requires_grad=True)\n",
    "# initialize context matrix\n",
    "W2 = Variable(to_device(torch.randn(vocabulary_size, embedding_dims), device), requires_grad=True)\n",
    "\n",
    "loss_val = 0\n",
    "\n",
    "\n",
    "for epo in range(num_epochs):    \n",
    "    s_time = time.time()\n",
    "    np.random.shuffle(VECTORIZED_SENTENCES)\n",
    "    print(f\"shuffling of sentence tensor took: {time.time()-s_time} secs\")\n",
    "    \n",
    "    for sent_idx, sentence in enumerate(VECTORIZED_SENTENCES):\n",
    "        \n",
    "        if sent_idx!=0 and sent_idx%100000==0:            \n",
    "            print(f\"processing {sent_idx}th sentence\")\n",
    "            # break\n",
    "            \n",
    "        for target_position, target_idx in enumerate(sentence):\n",
    "            # target_idx = word2idx[target_word]\n",
    "            context_indices = [sentence[idx] for idx in range(max(0, target_position - window_size), min(target_position + window_size + 1, len(sentence))) if idx != target_position]\n",
    "                # print(f\"target_word:{target_word}, context:{context}\")            \n",
    "            if not context_indices:\n",
    "                continue\n",
    "            # context_indices = [word2idx[word] for word in context]\n",
    "            \n",
    "            for context_idx in context_indices:                               \n",
    "                \n",
    "                #1.forward pass\n",
    "                x = get_input_layer(target_idx)\n",
    "                # y_true = Variable(to_device(torch.tensor([context_idx]).long(), device))\n",
    "                \n",
    "                # print(f\"x:{x}\")\n",
    "                # print(f\"y_true:{y_true}\")                \n",
    "                # generate positive score for target_context pair\n",
    "                \n",
    "                # get target word embedding\n",
    "                target_embedding = torch.matmul(W1, x)\n",
    "                # print(f\"target_embedding.shape: {target_embedding.shape}\")\n",
    "                \n",
    "                # get context word embedding\n",
    "                p_context_embedding = W2[context_idx]\n",
    "                # print(f\"p_context_embedding.shape: {p_context_embedding.shape}\")\n",
    "                \n",
    "                # generate positive score vector by dot product with other context vectors\n",
    "                positive_score = torch.sum(target_embedding * p_context_embedding)                \n",
    "                positive_score = positive_score.unsqueeze(0)\n",
    "                # print(positive_score)\n",
    "                \n",
    "                # sample 4 negative context words\n",
    "                negative_samples_idx = to_device(torch.tensor(random.choices(neg_sampling_candidates, k=negative_samples)), device)\n",
    "                # print(f\"negative_samples_idx: {negative_samples_idx}\")\n",
    "                \n",
    "                # get #negative_sample context vectors\n",
    "                negative_context_vectors = W2[negative_samples_idx]\n",
    "                # print(f\"negative_context_vectors.shape: {negative_context_vectors.shape}\")\n",
    "                \n",
    "                # generate negative score vector by dot product with other negative sample context vectors\n",
    "                negative_sample_scores = torch.matmul(negative_context_vectors, target_embedding)                \n",
    "                # print(f\"negative_scores: {negative_scores}\")\n",
    "                \n",
    "                # concatenate positive and negative scores\n",
    "                observed_scores = torch.cat((positive_score, negative_sample_scores))\n",
    "                # print(f\"observed_scores: {observed_scores}\")\n",
    "                \n",
    "                # concatenate corresponding desired output of one 1 and #negative_sample 0s\n",
    "                y_true = to_device(torch.concat((torch.ones(1), torch.zeros(negative_samples))), device)\n",
    "                # print(f\"y_true: {y_true}\")\n",
    "                \n",
    "                \n",
    "                # loss calculation\n",
    "                loss = loss_fn(observed_scores, y_true)\n",
    "                loss_val += loss.item()\n",
    "                loss.backward()\n",
    "                \n",
    "                # optimization\n",
    "                W1.data -= learning_rate * W1.grad.data\n",
    "                W2.data -= learning_rate * W2.grad.data\n",
    "                \n",
    "                W1.grad.data.zero_()\n",
    "                W2.grad.data.zero_()        \n",
    "    print(f'Loss at epo {epo}: {loss_val}')\n",
    "    print(f\"epoch#{epo} took {time.time()-s_time} secs\")\n",
    "\n",
    "# save weights \n",
    "torch.save(W1, \"W1_sg_ns.pt\")\n",
    "torch.save(W2, \"W2_sg_ns.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cbow with negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cbow with negative sampling\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "embedding_dims = 100\n",
    "num_epochs = 5\n",
    "learning_rate = 0.025\n",
    "window_size = 5\n",
    "negative_samples = 5\n",
    "\n",
    "\n",
    "\n",
    "# initialize target matrix\n",
    "W1 = Variable(to_device(torch.randn(embedding_dims,vocabulary_size), device), requires_grad=True)\n",
    "# initialize context matrix\n",
    "W2 = Variable(to_device(torch.randn(vocabulary_size, embedding_dims), device), requires_grad=True)\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD([W1, W2], lr=learning_rate)\n",
    "\n",
    "\n",
    "loss_val = 0\n",
    "for epo in range(num_epochs):\n",
    "    \n",
    "    # s_time = time.time()\n",
    "    np.random.shuffle(VECTORIZED_SENTENCES)\n",
    "    # print(f\"shuffling of sentence tensor took: {time.time()-s_time} secs\")\n",
    "    \n",
    "    #optimizer.zero_grad()  # Clear gradients before the loop\n",
    "    s_time = time.time()\n",
    "    for sent_idx, sentence in enumerate(VECTORIZED_SENTENCES):\n",
    "        # print(sentence)\n",
    "        \n",
    "        if sent_idx%1000 ==0:\n",
    "            print(f\"processed {sent_idx}th sentence\")\n",
    "        \n",
    "        # sentence_on_hots = get_one_hots_by_index(sentence)\n",
    "        for target_position, target_idx  in enumerate(sentence):\n",
    "            # target_idx = word2idx[target_word]\n",
    "            context_indices = [sentence[idx] for idx in range(max(0, target_position - window_size), min(target_position + window_size + 1, len(sentence))) if idx != target_position]\n",
    "            # print(f\"target_idx:{target_idx}, context:{context_indices}\")            \n",
    "            if not context_indices:\n",
    "                continue\n",
    "            \n",
    "            # get target embedding\n",
    "            x =  get_input_layer(target_idx)\n",
    "            # print(target_one_hot)\n",
    "            target_embedding = torch.matmul(W1, x)\n",
    "            # print(f\"target_embedding.shape: {target_embedding.shape}\")\n",
    "            \n",
    "            # get context words embedding\n",
    "            \n",
    "            context_embeds = W2[[context_indices]]\n",
    "            # print(f\"context_embeds:{context_embeds}\")\n",
    "            # print(f\"context_vectors.shape:{context_vectors.shape}\")\n",
    "            context_mean = torch.mean(context_embeds, dim=0)\n",
    "            # print(f\"context_mean.shape:{context_mean.shape}\")\n",
    "            # print(f\"context_mean:{context_mean}\")\n",
    "            \n",
    "            # get positive score\n",
    "            positive_score = torch.sum(target_embedding * context_mean)\n",
    "            positive_score = positive_score.unsqueeze(0)\n",
    "            # print(f\"positive_score: {positive_score}\")\n",
    "            \n",
    "\n",
    "#             # sample 4 negative context words\n",
    "            negative_samples_idx = to_device(torch.tensor(random.choices(neg_sampling_candidates, k=negative_samples)), device)\n",
    "            # print(f\"negative_samples_idx: {negative_samples_idx}\")\n",
    "\n",
    "#             # get #negative_sample context vectors\n",
    "            negative_context_vectors = W2[negative_samples_idx]\n",
    "            # print(f\"negative_context_vectors.shape: {negative_context_vectors.shape}\")\n",
    "            negative_context_mean = torch.mean(negative_context_vectors, dim=0)\n",
    "            # print(f\"negative_context_mean.shape: {negative_context_mean.shape}\")\n",
    "            negative_score = torch.sum(target_embedding * negative_context_mean)\n",
    "            negative_score = negative_score.unsqueeze(0)\n",
    "            \n",
    "            # concatenate positive and negative scores\n",
    "            observed_scores = torch.cat((positive_score, negative_score))\n",
    "            # print(f\"observed_scores: {observed_scores}\")\n",
    "\n",
    "            # concatenate corresponding desired output of one 1 and #negative_sample 0s\n",
    "            y_true = to_device(torch.tensor((torch.ones(1), torch.zeros(1))), device)\n",
    "            # print(f\"y_true: {y_true}\")\n",
    "            \n",
    "            \n",
    "            # loss calculation\n",
    "            loss = loss_fn(observed_scores, y_true)\n",
    "            # print(f\"loss: {loss}\")\n",
    "            loss_val += loss.item()\n",
    "            # Backpropagate and update weights\n",
    "            # s_time = time.time()\n",
    "            loss.backward()            \n",
    "            optimizer.step()\n",
    "        if sent_idx%1000 ==0:\n",
    "            print(f\"sentence#{sent_idx} took: {time.time()- s_time} secs\")\n",
    "            s_time = time.time()\n",
    "            \n",
    "            \n",
    "    print(f'Loss at epo {epo}: {loss_val}')\n",
    "    print(f\"epoch#{epo} took {time.time()-s_time} secs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# W1 = torch.load(\"W1.pt\")\n",
    "\n",
    "def similarity(w1, w2):\n",
    "    # find one hot vectors of w1 and w2\n",
    "    w1_one_hot = get_input_layer(word2idx[w1])\n",
    "    w2_one_hot = get_input_layer(word2idx[w2])\n",
    "    # get actual embeddings of words by multiplying one_hot with weight matrix\n",
    "    w1_embedding = torch.matmul(W1, w1_one_hot)\n",
    "    w2_embedding = torch.matmul(W1, w2_one_hot)\n",
    "    # find similarity between embeddings\n",
    "    return torch.dot(w1_embedding, w2_embedding) / (torch.norm(w1_embedding) * torch.norm(w2_embedding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = torch.matmul(W1.T, W1)\n",
    "# print(idx2word)\n",
    "\n",
    "def get_top_similar_words(word, similarity_matrix, top_n=10):\n",
    "    word_index = word2idx[word]\n",
    "    word_similarity = similarity_matrix[word_index]\n",
    "    top_n_similar_words = torch.argsort(word_similarity)[::][-top_n+1:]    \n",
    "    # omit the word itself\n",
    "    # return [vectorizer.get_feature_names_out()[i] for i in top_n_similar_words[1:]]\n",
    "    # print(top_n_similar_words)\n",
    "    return [idx2word[idx.item()] for idx in top_n_similar_words][::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_similar_words(\"Draupadi\", similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions\n",
    "# 1. Why two matrices required one for target another for context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
