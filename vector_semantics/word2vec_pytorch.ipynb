{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:14:38.054774748Z",
     "start_time": "2023-08-30T07:14:37.255421179Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "device = get_default_device()\n",
    "print(device)\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "al_regex = re.compile(r\"[^a-zA-Z]\")\n",
    "\n",
    "\n",
    "class MBCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = \"data/mahabharat_gutenberg_lemmatized_sents.txt\"\n",
    "        with open(corpus_path) as fp:\n",
    "            for line in fp.readlines():\n",
    "                tokens = line.split()\n",
    "                tokens = [al_regex.sub('', token) for token in tokens]\n",
    "                yield tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mb_sents = MBCorpus()\n",
    "vocabulary = []\n",
    "for sentence in mb_sents:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(f\"vocabulary_size:{vocabulary_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).type(torch.cuda.FloatTensor)\n",
    "    x[word_idx] = 1.0\n",
    "    \n",
    "    return to_device(x, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example of target with class indices\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randint(5, (3,), dtype=torch.int64)\n",
    "loss = F.cross_entropy(input, target)\n",
    "loss.backward()\n",
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "print(input)\n",
    "print(target)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "loss = F.cross_entropy(input, target)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:14:29.485418854Z",
     "start_time": "2023-08-30T07:14:29.479921397Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "embedding_dims = 100\n",
    "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).type(torch.cuda.FloatTensor), requires_grad=True)\n",
    "# to_device(W1, device)\n",
    "W2 = Variable(torch.randn(vocabulary_size, embedding_dims).type(torch.cuda.FloatTensor), requires_grad=True)\n",
    "# to_device(W2, device)\n",
    "num_epochs = 101\n",
    "learning_rate = 0.1\n",
    "window_size = 2\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    s_time = time.time()\n",
    "    loss_val = 0\n",
    "    # for each sentence\n",
    "    for sent_idx, sentence in enumerate(mb_sents):\n",
    "        if sent_idx!=0 and sent_idx%100000==0:            \n",
    "            print(f\"processing {sent_idx}th sentence\")\n",
    "            # break\n",
    "            \n",
    "        for idx, target_word in enumerate(sentence):\n",
    "            target_idx = word2idx[target_word]\n",
    "            context = [sentence[idx] for idx in range(max(0, target_idx - window_size), min(target_idx + window_size + 1, len(sentence))) if idx != target_idx]\n",
    "                # print(f\"target_word:{target_word}, context:{context}\")            \n",
    "            if not context:\n",
    "                continue\n",
    "            context_indices = [word2idx[word] for word in context]\n",
    "            \n",
    "            for context_idx in context_indices:                \n",
    "                x = Variable(get_input_layer(target_idx))\n",
    "                # y_true = Variable(torch.from_numpy(np.array([context_idx])).long())\n",
    "                y_true = Variable(get_input_layer(context_idx))\n",
    "                # print(y_true)\n",
    "                # print(f\"W1*x: {W1.shape}*{x.shape}\")\n",
    "                z1 = torch.matmul(W1, x)\n",
    "                # print(f\"W2*z1: {W2.shape}*{z1.shape}\")\n",
    "                z2 = torch.matmul(W2, z1)\n",
    "                # make z2 look like one-hot via softmax and then setting max probability to 1\n",
    "                # z2_soft_max = F.softmax(z2, dim=0)\n",
    "                # z2_one_hot = torch.zeros_like(z2_soft_max).type(torch.cuda.FloatTensor)\n",
    "                # z2_one_hot[torch.argmax(z2_soft_max)] = 1\n",
    "                \n",
    "                \n",
    "                # calculate softmax of z2\n",
    "                sf_z2 = F.softmax(z2, dim=0)\n",
    "                # print(f\"sf_z2.shape: {sf_z2.shape}\")\n",
    "                # print(f\"y_true.shape: {y_true.shape}\")\n",
    "                # apply cross entropy loss\n",
    "                # print()\n",
    "                loss = F.cross_entropy(sf_z2, y_true) #\n",
    "                \n",
    "                \n",
    "                # print(f\"z2: {z2}\")\n",
    "                # print(f\"z2.shape: {z2.shape}\")\n",
    "                # print(f\"z2: {z2}\")\n",
    "                # log_softmax = F.log_softmax(z2, dim=0)\n",
    "                # print(f\"log_softmax: {log_softmax}\")\n",
    "                # loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "                loss_val += loss.item()\n",
    "                loss.backward()\n",
    "                W1.data -= learning_rate * W1.grad.data\n",
    "                W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "                W1.grad.data.zero_()\n",
    "                W2.grad.data.zero_()\n",
    "    print(f'Loss at epo {epo}: {loss_val}')\n",
    "    print(f\"epoch#{epo} took {time.time()-s_time} secs\")\n",
    "        # get corresponding context embeddings\n",
    "        # for each target, context pair train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights \n",
    "torch.save(W1, \"W1.pt\")\n",
    "torch.save(W2, \"W2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "W1 = torch.load(\"W1.pt\")\n",
    "\n",
    "def similarity(w1, w2):\n",
    "    # find one hot vectors of w1 and w2\n",
    "    w1_one_hot = get_input_layer(word2idx[w1])\n",
    "    w2_one_hot = get_input_layer(word2idx[w2])\n",
    "    # get actual embeddings of words by multiplying one_hot with weight matrix\n",
    "    w1_embedding = torch.matmul(W1, w1_one_hot)\n",
    "    w2_embedding = torch.matmul(W1, w2_one_hot)\n",
    "    # find similarity between embeddings\n",
    "    return torch.dot(w1_embedding, w2_embedding) / (torch.norm(w1_embedding) * torch.norm(w2_embedding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = torch.matmul(W1.T, W1)\n",
    "# print(idx2word)\n",
    "\n",
    "def get_top_similar_words(word, similarity_matrix, top_n=10):\n",
    "    word_index = word2idx[word]\n",
    "    word_similarity = similarity_matrix[word_index]\n",
    "    top_n_similar_words = torch.argsort(word_similarity)[::][-top_n+1:]    \n",
    "    # omit the word itself\n",
    "    # return [vectorizer.get_feature_names_out()[i] for i in top_n_similar_words[1:]]\n",
    "    # print(top_n_similar_words)\n",
    "    return [idx2word[idx.item()] for idx in top_n_similar_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tryambaka',\n",
       " 'tribulation',\n",
       " 'perpetuator',\n",
       " 'boat',\n",
       " 'lucky',\n",
       " 'tamas',\n",
       " 'echo',\n",
       " 'Dhirga',\n",
       " 'Draupadi']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_similar_words(\"Draupadi\", similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions\n",
    "# 1. What if I replace softmaxed probabilities with 1-0 vector\n",
    "# a. Is it possible?\n",
    "# b. How is learning effected?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
