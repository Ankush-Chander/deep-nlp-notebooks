{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:14:38.054774748Z",
     "start_time": "2023-08-30T07:14:37.255421179Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "device = get_default_device()\n",
    "print(device)\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "al_regex = re.compile(r\"[^a-zA-Z]\")\n",
    "\n",
    "\n",
    "class MBCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = \"data/mahabharat_gutenberg_lemmatized_sents.txt\"\n",
    "        with open(corpus_path) as fp:\n",
    "            for line in fp.readlines():\n",
    "                tokens = line.split()\n",
    "                tokens = [al_regex.sub('', token) for token in tokens]\n",
    "                yield tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary_size:16456\n"
     ]
    }
   ],
   "source": [
    "# vocabulary management\n",
    "#1. prepare word-idx dictionary, reverse dictionary\n",
    "#2. proabability table for negative sampling\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "mb_sents = MBCorpus()\n",
    "word_freq = defaultdict(int)\n",
    "for sentence in mb_sents:\n",
    "    for token in sentence:\n",
    "            word_freq[token]+=1\n",
    "\n",
    "# print(f\"word_freq: {word_freq}\")\n",
    "vocabulary = sorted(word_freq.keys())            \n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "\n",
    "\n",
    "total_count = sum([count for count in word_freq.values()])\n",
    "word_probabilities = [float(word_freq[idx2word[idx]]/total_count) for idx in range(vocabulary_size)]\n",
    "print(f\"vocabulary_size:{vocabulary_size}\")\n",
    "# print(sum(word_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramBatchModel(torch.nn.Module):\n",
    "    \"\"\" Center word as input, context words as target.\n",
    "        Objective is to maximize the score of map from input to target.\n",
    "    \"\"\"\n",
    "    def __init__(self, device, vocabulary_size, embedding_dim, neg_num=5, word_count=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        super(SkipGramBatchModel, self).__init__()\n",
    "        if word_count is None:\n",
    "            word_count = []\n",
    "        self.device = device\n",
    "        self.neg_num = neg_num\n",
    "        self.target_embeddings = torch.nn.Embedding(vocabulary_size, embedding_dim)\n",
    "        self.context_embeddings = torch.nn.Embedding(vocabulary_size, embedding_dim)\n",
    "        initrange = 0.5 / embedding_dim\n",
    "        self.target_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.context_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        # if self.neg_num > 0:\n",
    "        #     self.table = create_sample_table(word_count)\n",
    "\n",
    "    def forward(self, centers, contexts):\n",
    "        # print(f\"batch_size: {batch_size}\")\n",
    "        # print(f\"self.target_embeddings.shape: {self.target_embeddings.shape}\")\n",
    "        u_embeds = self.target_embeddings(centers)  \n",
    "        # print(f\"u_embeds.shape: {u_embeds.shape}\")        \n",
    "        v_embeds = self.context_embeddings(contexts)\n",
    "        # print(f\"v_embeds.shape: {v_embeds.shape}\")\n",
    "        # convert context embeddings from row vectors to column vectors via transpose\n",
    "        scores = torch.bmm(u_embeds, v_embeds.transpose(1,2)).squeeze()        \n",
    "        return scores \n",
    "\n",
    "    def get_embeddings(self):\n",
    "        return self.target_embeddings.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████▎                                 | 100008/305796 [16:09<41:39, 82.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 100000th sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|████████████████████████████████                 | 200014/305796 [32:14<15:24, 114.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 200000th sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████████████████████████████████████████████ | 300018/305796 [48:18<01:04, 89.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 300000th sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 305796/305796 [49:12<00:00, 103.59it/s]\n",
      " 20%|███████████▏                                            | 1/5 [49:12<3:16:48, 2952.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss at step 0: 2508078.0879694223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████▎                                 | 100009/305796 [16:06<37:22, 91.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 100000th sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|████████████████████████████████                 | 200020/305796 [32:13<14:41, 119.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 200000th sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████████████████████████████████████████████ | 300018/305796 [48:19<00:57, 99.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 300000th sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 305796/305796 [49:14<00:00, 103.48it/s]\n",
      " 40%|█████████████████████▌                                | 2/5 [1:38:27<2:27:41, 2953.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss at step 1: 2400102.585960269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████▎                                 | 100003/305796 [15:58<35:39, 96.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 100000th sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|████████████████████████████████                 | 200020/305796 [32:12<14:13, 124.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 200000th sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████████████████████████████████████████████████ | 300017/305796 [48:19<00:56, 102.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 300000th sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 305796/305796 [49:15<00:00, 103.47it/s]\n",
      " 60%|████████████████████████████████▍                     | 3/5 [2:27:42<1:38:28, 2954.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss at step 2: 2322464.0502568483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████▎                                 | 100003/305796 [15:52<35:12, 97.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 100000th sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|████████████████████████████████                 | 200020/305796 [31:45<14:22, 122.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 200000th sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████████████████████████████████████████████ | 300018/305796 [47:55<00:59, 97.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 300000th sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 305796/305796 [48:51<00:00, 104.32it/s]\n",
      " 80%|████████████████████████████████████████████▊           | 4/5 [3:16:33<49:05, 2945.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss at step 3: 2274707.5015500784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████▎                                 | 100008/305796 [15:56<41:31, 82.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 100000th sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|████████████████████████████████                 | 200017/305796 [31:52<14:06, 124.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 200000th sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████████████████████████████████████████████████ | 300017/305796 [47:55<00:54, 105.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 300000th sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 305796/305796 [48:49<00:00, 104.39it/s]\n",
      "100%|████████████████████████████████████████████████████████| 5/5 [4:05:22<00:00, 2944.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss at step 4: 2241598.600239277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "from tqdm import tqdm\n",
    "embedding_dim = 100\n",
    "num_epochs = 5\n",
    "learning_rate = 0.025\n",
    "window_size = 5\n",
    "neg_num = 5\n",
    "word_count = None\n",
    "batch_size=128\n",
    "\n",
    "\n",
    "vocabulary_tensor = torch.arange(len(vocabulary)).to(device)  # Assuming vocabulary is a range or similar\n",
    "word_probabilities_tensor = torch.Tensor(word_probabilities).to(device)\n",
    "\n",
    "\n",
    "\n",
    "def generate_batch(data, batch_size=128, neg_num=4):\n",
    "    \"\"\"\n",
    "    generates batch_size number of  targets, batch_size * (1 positive + neg_num negative) contexts \n",
    "    \"\"\"\n",
    "    # centers, contexts = [], []\n",
    "    with tqdm(total=305796, position=0, leave=True) as pbar:\n",
    "        centers, contexts=[], []\n",
    "        for sent_idx, sentence in enumerate(data):\n",
    "            sentence = [word2idx[word] for word in sentence]\n",
    "\n",
    "            if sent_idx!=0 and sent_idx%100000==0:            \n",
    "                print(f\"processing {sent_idx}th sentence\")\n",
    "                # break\n",
    "\n",
    "            for target_position, target_idx in enumerate(sentence):\n",
    "                context_indices = [sentence[idx] for idx in range(max(0, target_position - window_size), min(target_position + window_size + 1, len(sentence))) if idx != target_position]\n",
    "\n",
    "\n",
    "                if not context_indices:\n",
    "                    continue            \n",
    "                #print(f\"target_idx:{target_idx}, context_indices:{context_indices}\")\n",
    "\n",
    "                for context_idx in context_indices:\n",
    "                    centers.append([target_idx])\n",
    "                    p_contexts = [context_idx]\n",
    "                    # n_contexts = np.random.choice(len(vocabulary), neg_num, p=word_probabilities).tolist()\n",
    "                    n_contexts = torch.multinomial(word_probabilities_tensor, neg_num, replacement=True).cpu().tolist()\n",
    "\n",
    "                    # print(f\"{len(p_contexts)}+{len(n_contexts)} = {len(p_contexts + n_contexts)}\")\n",
    "                    contexts.append(p_contexts + n_contexts)\n",
    "                    \n",
    "                    # add ne\n",
    "                    if len(centers) == batch_size:\n",
    "                        yield torch.LongTensor(centers).to(device), torch.LongTensor(contexts).to(device)\n",
    "                        centers, contexts = [], [] \n",
    "            # break\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "def train_batch(clip=1.0, batch_size=128, neg_num=4):\n",
    "    batch_model = SkipGramBatchModel(device=device, vocabulary_size=vocabulary_size, embedding_dim=embedding_dim, neg_num=neg_num)\n",
    "    batch_model.to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        batch_model.parameters(), lr=learning_rate)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for i in tqdm(range(num_epochs)):\n",
    "        loss_val= 0\n",
    "        for j, (centers, contexts) in enumerate(generate_batch(mb_sents, batch_size=batch_size, neg_num=neg_num)):\n",
    "            y_pred = batch_model(centers, contexts)\n",
    "            y_true = [[1] + [0]*neg_num]*batch_size\n",
    "            y_true = torch.FloatTensor(y_true).to(device)\n",
    "            \n",
    "            loss = loss_fn(y_pred, y_true)\n",
    "            loss_val += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(batch_model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            # Print loss value at certain step\n",
    "            loss_val += loss.item()\n",
    "            \n",
    "        print(f\"Total loss at step {i}: {loss_val}\")\n",
    "        loss_val = 0\n",
    "    return batch_model.get_embeddings()\n",
    "\n",
    "# x = train()\n",
    "x = train_batch(batch_size=128)\n",
    "\n",
    "# %lprun -f generate_batch list(generate_batch(mb_sents,batch_size=2))\n",
    "# %lprun -f train_batch train_batch(batch_size=2)\n",
    "\n",
    "# todo: fix average loss\n",
    "# think batch variant: done\n",
    "# profile batch varient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving embeddings to file: batchwise_embeddings.bin\n"
     ]
    }
   ],
   "source": [
    "# saving embeddings\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def save_embeddings(filename=\"embeddings.bin\", embeddings=None, dictionary=None):\n",
    "    \"\"\"Embeddings and reverse dictionary serialization and dump to a file.\"\"\"\n",
    "    data = {\n",
    "        'emb': embeddings,\n",
    "        'dict': dictionary\n",
    "    }\n",
    "    file = open(filename, 'wb')\n",
    "    print(\"Saving embeddings to file:\", filename)\n",
    "    pickle.dump(data, file)\n",
    "\n",
    "save_embeddings(filename=\"batchwise_embeddings.bin\", embeddings=x, dictionary=word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arjuna: [('bowl', 571.06976), ('residential', 464.91788), ('Erecting', 419.4904), ('adamantine', 411.66492), ('Yugandharah', 392.29828), ('pleasest', 391.912), ('interior', 388.27072), ('Sarvamedha', 381.7625), ('hillock', 378.05655), ('reverent', 376.54163)]\n",
      "Drona: [('knotty', 651.8836), ('madhava', 646.5299), ('Jatayu', 612.64), ('seer', 611.40985), ('Drona', 595.5215), ('observeth', 586.3527), ('wily', 564.8399), ('wearer', 559.83325), ('Punaravarta', 553.14795), ('Pratyagraha', 537.84247)]\n",
      "Bhishma: [('Nagadatta', 343.24554), ('Chandanas', 287.1361), ('destroyest', 270.96735), ('Adityaketu', 269.16922), ('Vudvuda', 256.75308), ('peel', 249.93112), ('Anuyaina', 236.2539), ('Regenerate', 229.32101), ('srutasravasthese', 226.7105), ('chiefship', 223.22006)]\n",
      "Krishna: [('Mahodara', 287.5843), ('vishnu', 251.88232), ('saidThe', 236.85614), ('pallour', 235.41672), ('gnaw', 234.51636), ('deficiency', 228.48544), ('Saha', 225.25494), ('Sarpa', 223.18712), ('Bhimaeara', 214.36134), ('Samyava', 212.11496)]\n",
      "mace: [('adamantine', 1216.2185), ('mace', 1098.2676), ('Bhuti', 1068.2494), ('squeeze', 982.41504), ('shot', 942.4571), ('emblic', 927.8486), ('contemptible', 908.8754), ('again', 869.0441), ('Viravahu', 851.75354), ('forms', 826.99866)]\n",
      "Karna: [('Kuthara', 836.27704), ('Durdharsha', 757.2405), ('Dari', 739.72437), ('criminal', 728.8531), ('Sumuksha', 728.6585), ('Shechaka', 687.56854), ('madhava', 684.2189), ('Sarvasaranga', 681.7809), ('Vyaya', 663.3533), ('warriorsBhishma', 630.0054)]\n",
      "Pandu: [('said', 601.58435), ('marvellous', 538.7685), ('annoyed', 531.8955), ('likeness', 519.54944), ('distort', 517.1194), ('Jatarupam', 512.45276), ('expatiate', 512.1478), ('Wonderful', 509.61856), ('Pandu', 485.43323), ('Saraswat', 478.63306)]\n",
      "Kunti: [('center', 509.12903), ('unkindness', 460.7352), ('deficiency', 445.9355), ('impalement', 416.39948), ('outwardly', 411.37778), ('Avak', 403.6161), ('miserably', 396.5743), ('onward', 394.42587), ('Aushija', 389.7923), ('Akritavrana', 381.8544)]\n",
      "Yudhishthira: [('Iravati', 286.51904), ('dadhichi', 271.32944), ('Devika', 270.7703), ('Gokarna', 264.43463), ('obtaineth', 258.92267), ('Rukmi', 252.7208), ('Himvat', 252.56879), ('Gopati', 251.54971), ('revivify', 250.33269), ('malignity', 250.23157)]\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "class Word2Vec(object):\n",
    "    \"\"\"Inference interface of Word2Vec embeddings\n",
    "        Before inference the embdedding result of a word, data need to be initialized\n",
    "        by calling method from_file or from_object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.embeddings = None\n",
    "        self.dictionary = None\n",
    "        self.reverse_dictionary = None\n",
    "\n",
    "    def from_file(self, filename):\n",
    "        file = open(filename, 'rb')\n",
    "        data = pickle.load(file)\n",
    "        self.embeddings = data['emb']\n",
    "        self.dictionary = data['dict']\n",
    "        self.reverse_dictionary = {v:k for k,v in self.dictionary.items()}\n",
    "\n",
    "    def from_object(self, embeddings, dictionary):\n",
    "        self.embeddings = embeddings\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "    def inference(self, word):\n",
    "        assert self.embeddings is not None and self.dictionary is not None, \\\n",
    "            'Embeddings not initialized, use from_file or from_object to load data.'\n",
    "        word_idx = self.dictionary.get(word)\n",
    "        # Unknown word returns UNK's word_idx\n",
    "        if word_idx is None:\n",
    "            word_idx = 0\n",
    "        return self.embeddings[word_idx]\n",
    "\n",
    "    def similarity(self, word1, word2):\n",
    "        v1 = self.inference(word1)\n",
    "        v1 = v1.cpu().numpy()\n",
    "        v2 = self.inference(word2)\n",
    "        v2 = v2.cpu().numpy()\n",
    "        # perform cosine similarity using torch\n",
    "        return np.dot(v1, v2)  # / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "    def most_similar(self, word, topk=10):\n",
    "        assert self.embeddings is not None and self.dictionary is not None, \\\n",
    "            'Embeddings not initialized, use from_file or from_object to load data.'\n",
    "        word_idx = self.dictionary.get(word)\n",
    "        # Unknown word returns UNK's word_idx\n",
    "        if word_idx is None:\n",
    "            word_idx = 0\n",
    "        word_emb = self.embeddings[word_idx].cpu().numpy()\n",
    "        # word_emb = word_emb / np.linalg.norm(word_emb)\n",
    "        similarity = np.dot(self.embeddings.cpu().numpy(), word_emb)\n",
    "        sorted_idx = np.argsort(similarity)[::-1]\n",
    "        return [(self.reverse_dictionary.get(idx, \"\"), similarity[idx]) for idx in sorted_idx[:topk]]\n",
    "\n",
    "    def analogy(self, word1, word2, word3, topk=10):\n",
    "        assert self.embeddings is not None and self.dictionary is not None, \\\n",
    "            'Embeddings not initialized, use from_file or from_object to load data.'\n",
    "        word1_idx = self.dictionary.get(word1, 0)\n",
    "        word2_idx = self.dictionary.get(word2, 0)\n",
    "        word3_idx = self.dictionary.get(word3, 0)\n",
    "        # Unknown word returns UNK's word_idx\n",
    "\n",
    "        word1_emb = self.embeddings[word1_idx].cpu().numpy()\n",
    "        word2_emb = self.embeddings[word2_idx].cpu().numpy()\n",
    "        word3_emb = self.embeddings[word3_idx].cpu().numpy()\n",
    "        word4_emb = word2_emb - word1_emb + word3_emb\n",
    "        similarity = np.dot(self.embeddings.cpu().numpy(), word4_emb)\n",
    "        sorted_idx = np.argsort(similarity)[::-1]\n",
    "        return [(self.reverse_dictionary.get(idx, \"\"), similarity[idx]) for idx in sorted_idx[:topk]]\n",
    "\n",
    "    \n",
    "wv = Word2Vec()\n",
    "wv.from_file(\"batchwise_embeddings.bin\")\n",
    "for word in [\"Arjuna\", \"Drona\",\"Bhishma\", \"Krishna\", \"mace\", \"Karna\", \"Pandu\", \"Kunti\", \"Yudhishthira\"]:\n",
    "    print(f\"{word}: {wv.most_similar(word)}\")\n",
    "# wv.most_similar(\"Arjuna\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
