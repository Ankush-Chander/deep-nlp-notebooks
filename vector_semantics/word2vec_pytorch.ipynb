{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:14:38.054774748Z",
     "start_time": "2023-08-30T07:14:37.255421179Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "device = get_default_device()\n",
    "print(device)\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "al_regex = re.compile(r\"[^a-zA-Z]\")\n",
    "\n",
    "\n",
    "class MBCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = \"data/mahabharat_gutenberg_lemmatized_sents.txt\"\n",
    "        with open(corpus_path) as fp:\n",
    "            for line in fp.readlines():\n",
    "                tokens = line.split()\n",
    "                tokens = [al_regex.sub('', token) for token in tokens]\n",
    "                yield tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary_size:16456\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mb_sents = MBCorpus()\n",
    "vocabulary = []\n",
    "for sentence in mb_sents:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(f\"vocabulary_size:{vocabulary_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).type(torch.FloatTensor)\n",
    "    x[word_idx] = 1.0\n",
    "    \n",
    "    return to_device(x, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example of target with class indices\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randint(5, (3,), dtype=torch.int64)\n",
    "loss = F.cross_entropy(input, target)\n",
    "loss.backward()\n",
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "print(input)\n",
    "print(target)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "loss = F.cross_entropy(input, target)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:14:29.485418854Z",
     "start_time": "2023-08-30T07:14:29.479921397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 0: 1343892.9336804668\n",
      "epoch#0 took 160.4619016647339 secs\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 1: 1046405.2954747919\n",
      "epoch#1 took 166.35582566261292 secs\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 2: 965284.833564287\n",
      "epoch#2 took 166.0302722454071 secs\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 3: 927679.5668445043\n",
      "epoch#3 took 168.14582085609436 secs\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 4: 908387.2600252181\n",
      "epoch#4 took 170.9681613445282 secs\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 5: 898009.5184652656\n",
      "epoch#5 took 137.1023633480072 secs\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 6: 892298.7196095102\n",
      "epoch#6 took 119.26675629615784 secs\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 7: 889056.4096065238\n",
      "epoch#7 took 120.66339612007141 secs\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 8: 887222.4873125181\n",
      "epoch#8 took 120.64828276634216 secs\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 9: 886146.7582591586\n",
      "epoch#9 took 120.29722332954407 secs\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 10: 885362.9586564265\n",
      "epoch#10 took 119.83731174468994 secs\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 11: 884638.6971586645\n",
      "epoch#11 took 121.14107656478882 secs\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 12: 883979.0593496189\n",
      "epoch#12 took 120.02906346321106 secs\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 13: 883364.6081536822\n",
      "epoch#13 took 119.52970480918884 secs\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 14: 882922.176249437\n",
      "epoch#14 took 120.0471203327179 secs\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 15: 882336.9453232288\n",
      "epoch#15 took 120.78517985343933 secs\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 16: 881601.1994629726\n",
      "epoch#16 took 120.95428085327148 secs\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 17: 880940.3433254436\n",
      "epoch#17 took 120.75678825378418 secs\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 18: 880351.1427121311\n",
      "epoch#18 took 120.6239402294159 secs\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 19: 879857.5945067815\n",
      "epoch#19 took 121.15275120735168 secs\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 20: 879482.6966337115\n",
      "epoch#20 took 120.58685564994812 secs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "embedding_dims = 100\n",
    "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).type(torch.cuda.FloatTensor), requires_grad=True)\n",
    "# to_device(W1, device)\n",
    "W2 = Variable(torch.randn(vocabulary_size, embedding_dims).type(torch.cuda.FloatTensor), requires_grad=True)\n",
    "# to_device(W2, device)\n",
    "num_epochs = 101\n",
    "learning_rate = 0.1\n",
    "window_size = 2\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    s_time = time.time()\n",
    "    loss_val = 0\n",
    "    # for each sentence\n",
    "    for sent_idx, sentence in enumerate(mb_sents):\n",
    "        if sent_idx!=0 and sent_idx%100000==0:            \n",
    "            print(f\"processing {sent_idx}th sentence\")\n",
    "            # break\n",
    "            \n",
    "        for idx, target_word in enumerate(sentence):\n",
    "            target_idx = word2idx[target_word]\n",
    "            context = [sentence[idx] for idx in range(max(0, target_idx - window_size), min(target_idx + window_size + 1, len(sentence))) if idx != target_idx]\n",
    "                # print(f\"target_word:{target_word}, context:{context}\")            \n",
    "            if not context:\n",
    "                continue\n",
    "            context_indices = [word2idx[word] for word in context]\n",
    "            \n",
    "            for context_idx in context_indices:                \n",
    "                x = Variable(get_input_layer(target_idx))\n",
    "                y_true = Variable(to_device(torch.from_numpy(np.array([context_idx])).long(), device))\n",
    "                # y_true = Variable(get_input_layer(context_idx))\n",
    "                # print(y_true)\n",
    "                # print(f\"W1*x: {W1.shape}*{x.shape}\")\n",
    "                z1 = torch.matmul(W1, x)\n",
    "                # print(f\"W2*z1: {W2.shape}*{z1.shape}\")\n",
    "                z2 = torch.matmul(W2, z1)\n",
    "                log_softmax = F.log_softmax(z2, dim=0)\n",
    "                # print(f\"log_softmax: {log_softmax}\")\n",
    "                loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "                loss_val += loss.item()\n",
    "                loss.backward()\n",
    "                W1.data -= learning_rate * W1.grad.data\n",
    "                W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "                W1.grad.data.zero_()\n",
    "                W2.grad.data.zero_()\n",
    "    print(f'Loss at epo {epo}: {loss_val}')\n",
    "    print(f\"epoch#{epo} took {time.time()-s_time} secs\")\n",
    "        # get corresponding context embeddings\n",
    "        # for each target, context pair train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights \n",
    "torch.save(W1, \"W1.pt\")\n",
    "torch.save(W2, \"W2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# W1 = torch.load(\"W1.pt\")\n",
    "\n",
    "def similarity(w1, w2):\n",
    "    # find one hot vectors of w1 and w2\n",
    "    w1_one_hot = get_input_layer(word2idx[w1])\n",
    "    w2_one_hot = get_input_layer(word2idx[w2])\n",
    "    # get actual embeddings of words by multiplying one_hot with weight matrix\n",
    "    w1_embedding = torch.matmul(W1, w1_one_hot)\n",
    "    w2_embedding = torch.matmul(W1, w2_one_hot)\n",
    "    # find similarity between embeddings\n",
    "    return torch.dot(w1_embedding, w2_embedding) / (torch.norm(w1_embedding) * torch.norm(w2_embedding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = torch.matmul(W1.T, W1)\n",
    "# print(idx2word)\n",
    "\n",
    "def get_top_similar_words(word, similarity_matrix, top_n=10):\n",
    "    word_index = word2idx[word]\n",
    "    word_similarity = similarity_matrix[word_index]\n",
    "    top_n_similar_words = torch.argsort(word_similarity)[::][-top_n+1:]    \n",
    "    # omit the word itself\n",
    "    # return [vectorizer.get_feature_names_out()[i] for i in top_n_similar_words[1:]]\n",
    "    # print(top_n_similar_words)\n",
    "    return [idx2word[idx.item()] for idx in top_n_similar_words][::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_similar_words(\"Draupadi\", similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions\n",
    "# 1. What if I replace softmaxed probabilities with 1-0 vector\n",
    "# a. Is it possible?\n",
    "# b. How is learning effected?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
