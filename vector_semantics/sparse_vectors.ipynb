{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f13b2d-a0c5-47ed-b96e-436fcc9b3632",
   "metadata": {},
   "source": [
    "# Sparse vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315ccc6c-66dc-4cf2-afa0-7a6d9ee489a9",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Project:\n",
    "1. Generate TF-IDF matrix from Mahabharat text.\n",
    "\n",
    "    a. Section as a document\n",
    "\n",
    "    b. paragraph as a document\n",
    "\n",
    "    c. Calculate similarity of common words from the TFIDF matrix\n",
    "\n",
    "2. Generate term-term matrix weighted by PMI with co-occurance criteria as\n",
    "\n",
    "    a. section as a document\n",
    "\n",
    "    b. paragraph as a document\n",
    "    \n",
    "    c. calculate similarity of common words from the PMID matrix\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ea62c-e4c3-491e-a235-f823b5448ad4",
   "metadata": {},
   "source": [
    "# Text description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2902aa5c-83e6-415b-a250-7a67bb68deb6",
   "metadata": {},
   "source": [
    "Experiment text has been taken from [project gutenberg](https://www.gutenberg.org/ebooks/search/?query=The+Mahabharata+of+Krishna-Dwaipayana+Vyasa) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61394208546d9feb",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accbe4e0-06d3-49c0-ad83-d81856d3a427",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-21T05:05:52.638733661Z",
     "start_time": "2023-08-21T05:05:52.618583559Z"
    }
   },
   "outputs": [],
   "source": [
    "# get structured text from raw_text\n",
    "import ujson\n",
    "import time\n",
    "\n",
    "for volume in range(1,5):    \n",
    "    with open(f\"./data/mahabharat_gutenberg_vol{volume}.txt\") as fp:\n",
    "        begin, section_begin = False, False\n",
    "        para = \"\"\n",
    "        sections = []\n",
    "        section = None\n",
    "\n",
    "        with open(\"./data/mahabharat_gutenberg.jsonl\", \"a+\") as jsonl_fp:\n",
    "\n",
    "            for i, line in enumerate(fp.readlines()):\n",
    "                # ignore introductory sections\n",
    "                if not begin and line.strip() != \"THE MAHABHARATA\":\n",
    "                    continue\n",
    "\n",
    "                begin = True\n",
    "\n",
    "                # collect all sections as list of paras such that corpus is list of sections, sections is list of paragraphs\n",
    "                if line.startswith(\"SECTION \"):\n",
    "                    # write previous section to file\n",
    "                    if section:\n",
    "                        #\n",
    "                        # split section text to paragraphs.\n",
    "                        paragraphs = section[\"text\"].split(\"\\n\\n\")\n",
    "                        paragraphs = [para.strip().replace(\"\\n\", \" \") for para in paragraphs if\n",
    "                                      para and \"Parva continued\" not in para]\n",
    "\n",
    "                        section[\"paragraphs\"] = paragraphs\n",
    "                        # print(f\"{volume}:{section['SECTION_ID']}:{len(paragraphs)}\")\n",
    "                        del section[\"text\"]\n",
    "                        sections.append(section)\n",
    "                        jsonl_fp.write(f\"{ujson.dumps(section)}\\n\")\n",
    "                        # break\n",
    "                    section_begin = True\n",
    "                    section = {\"volume\":volume, \"SECTION_ID\": line.strip(), \"text\": \"\"}\n",
    "                else:\n",
    "                    # add line to section text\n",
    "                    if section:\n",
    "                        section[\"text\"] += line\n",
    "\n",
    "                # ignore after footnotes\n",
    "                if line.strip() == \"FOOTNOTES\":\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e6ade0dd3e5a98a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-21T05:05:33.414319799Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get documents from raw_text\n",
    "\n",
    "def get_sections_from_text(filepath: str) -> list:\n",
    "    section_wise_corpus = []\n",
    "    with open(filepath) as fp:\n",
    "        for line in fp.readlines():\n",
    "            section = ujson.loads(line)\n",
    "            section_text = \"\".join(section[\"paragraphs\"])\n",
    "            section_wise_corpus.append(section_text)\n",
    "    return section_wise_corpus\n",
    "\n",
    "\n",
    "def get_paragraphs_from_text(filepath: str) -> list:\n",
    "    para_wise_corpus = []\n",
    "    with open(filepath) as fp:\n",
    "        for line in fp.readlines():\n",
    "            section = ujson.loads(line)\n",
    "            paragraph_texts = section[\"paragraphs\"]\n",
    "            para_wise_corpus.extend(paragraph_texts)\n",
    "    return para_wise_corpus\n",
    "\n",
    "def get_sentences_from_text(filepath:str)->list:\n",
    "    sent_wise_corpus = []\n",
    "    with open(filepath) as fp:\n",
    "        for line in fp.readlines():\n",
    "            section = ujson.loads(line)\n",
    "            section_text = \"\".join(section[\"paragraphs\"])\n",
    "            section_doc = nlp(section_text)\n",
    "            section_sents = [sent.lemma_ for sent in section_doc]\n",
    "            sent_wise_corpus.extend(section_sents)\n",
    "    return sent_wise_corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bd0792005291b8",
   "metadata": {},
   "source": [
    "# TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1626b17a6e9fa318",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-21T05:05:33.414515210Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# similarity related functions\n",
    "# get top 10 similar words\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "al_regex = re.compile('[^a-zA-Z]')\n",
    "\n",
    "class SpacyLemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        text = text.encode('utf-8','ignore').decode(\"utf-8\")\n",
    "        lemmas =  [token.lemma_ for token in self.nlp(text) if not (token.is_stop or token.is_punct)]\n",
    "        # select only nouns\n",
    "        # lemmas =  [token.lemma_ for token in self.nlp(text) if token.pos_ in [\"VERB\"]]\n",
    "        \n",
    "        lemmas  = [al_regex.sub('', lemma) for lemma in lemmas]\n",
    "        lemmas  = [lemma.strip() for lemma in lemmas if lemma.strip()]\n",
    "        return lemmas\n",
    "\n",
    "\n",
    "def get_top_similar_words(word, similarity_matrix, vectorizer, top_n=10):\n",
    "    word_index = vectorizer.vocabulary_[word]\n",
    "    word_similarity = similarity_matrix[word_index]\n",
    "    top_n_similar_words = np.argsort(word_similarity)[::-1][:top_n+1]\n",
    "    # omit the word itself\n",
    "    return [vectorizer.get_feature_names_out()[i] for i in top_n_similar_words[1:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dd716bb-8e83-4f98-b340-5219ef40ba30",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-21T05:05:33.415472896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16045, 4998)\n",
      "cosine_similarity took:7.4619505405426025 secs\n",
      "krishna: ['vasudeva', 'kesava', 'gobardhana', 'armsbhishma', 'valin', 'brindavana', 'languishe', 'vyahritis', 'predestiny', 'absurd']\n",
      "mace: ['impetuously', 'spike', 'officiousness', 'gloved', 'uncharged', 'womenon', 'rush', 'kapittha', 'tumult', 'apple']\n",
      "bhishma: ['say', 'kuru', 'o', 'pandu', 'son', 'kurus', 'great', 'repel', 'santanu', 'excellentindeed']\n",
      "drona: ['pupil', 'bharadwaja', 'aswatthaman', 'prishata', 'weapon', 'aim', 'unafflicted', 'hiranyadhanus', 'defied', 'reproachingly']\n",
      "pandu: ['kunti', 'dhritarashtra', 'son', 'vaisampayana', 'kuru', 'bhishma', 'kurus', 'citizen', 'slope', 'madri']\n"
     ]
    }
   ],
   "source": [
    "#document: section\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=SpacyLemmaTokenizer())\n",
    "\n",
    "# get document-term matrix via vectorizer\n",
    "SECTION_WISE_CORPUS = get_sections_from_text(\"./data/mahabharat_gutenberg.jsonl\")\n",
    "\n",
    "document_term_matrix = vectorizer.fit_transform(SECTION_WISE_CORPUS)\n",
    "\n",
    "# transpose X to get term-document matrix\n",
    "term_document_matrix = document_term_matrix.T.toarray()\n",
    "print(term_document_matrix.shape)\n",
    "\n",
    "# get similarity of words\n",
    "s_time= time.time()\n",
    "section_doc_similarity_matrix = cosine_similarity(term_document_matrix)\n",
    "print(f\"cosine_similarity took:{time.time() - s_time} secs\")\n",
    "# print(section_doc_similarity_matrix.shape)\n",
    "\n",
    "\n",
    "for word in [\"krishna\", \"mace\", \"bhishma\", \"drona\", \"pandu\"]:\n",
    "    print(f\"{word}: {get_top_similar_words(word, section_doc_similarity_matrix, vectorizer)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae438d9c3a2ed7e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-21T05:05:33.457723203Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#document: paragraph\n",
    "#TFIDF based on paragraphs\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=SpacyLemmaTokenizer())\n",
    "\n",
    "\n",
    "# get document term matrix via vectorizer\n",
    "PARA_WISE_CORPUS = get_paragraphs_from_text(\"./data/mahabharat_gutenberg.jsonl\")\n",
    "\n",
    "document_term_matrix = vectorizer.fit_transform(PARA_WISE_CORPUS)\n",
    "\n",
    "# transpose X to get term-document matrix\n",
    "term_document_matrix = document_term_matrix.T.toarray()\n",
    "\n",
    "\n",
    "# get similarity of words\n",
    "s_time= time.time()\n",
    "para_doc_similarity_matrix = cosine_similarity(term_document_matrix)\n",
    "print(f\"cosine_similarity took:{time.time() - s_time} secs\")\n",
    "\n",
    "for word in [\"krishna\", \"mace\", \"bhishma\", \"drona\", \"pandu\"]:\n",
    "    print(f\"{word}: {get_top_similar_words(word, para_doc_similarity_matrix, vectorizer)}\")\n",
    "# get_top_similar_words(\"\", similarity_matrix, vectorizer)\n",
    "# OOM error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894036dc3fc4462c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-21T05:05:33.480809899Z",
     "start_time": "2023-08-21T05:05:33.457981827Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# document: sentence\n",
    "\n",
    "#document: paragraph\n",
    "#TFIDF based on sections without lemmatization\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# \n",
    "# vectorizer = TfidfVectorizer()\n",
    "# \n",
    "# para_wise_corpus = get_sentences_from_text(\"/home/ankush/workplace/nlp_projects/nlp-notebooks/vector_semantics/data/mahabharat_gutenberg.jsonl\")\n",
    "# \n",
    "# # get document term matrix via vectorizer\n",
    "# document_term_matrix = vectorizer.fit_transform(para_wise_corpus)\n",
    "# \n",
    "# # transpose X to get term document matrix\n",
    "# term_document_matrix = document_term_matrix.T.toarray()\n",
    "# print(term_document_matrix.shape)\n",
    "# print(vectorizer.get_feature_names_out())\n",
    "# \n",
    "# \n",
    "# # get similarity of words\n",
    "# \n",
    "# similarity_matrix = cosine_similarity(term_document_matrix)\n",
    "# print(similarity_matrix.shape)\n",
    "# \n",
    "# get_top_similar_words(\"drona\", similarity_matrix, vectorizer)\n",
    "\n",
    "# MemoryError: Unable to allocate 42.8 GiB for an array with shape (10579, 542676) and data type float64\n",
    "# TODO: how to handle out of memory matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff3a83c3f11d316",
   "metadata": {},
   "source": [
    "# PMI: Pointwise Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fb3d263-b15f-4153-bac6-6457fde306fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert co-occurence matrix to PMI matrix\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})\n",
    "\n",
    "def convert_coccurence_matrix_to_pmi(Xc):    \n",
    "    # Step 1: Calculate co-occurrence probabilities\n",
    "    total_word_occurrences = np.sum(Xc)\n",
    "    print(f\"total_word_occurrences:{total_word_occurrences}\")\n",
    "    \n",
    "    co_occurrence_probs = Xc / total_word_occurrences\n",
    "    #print(f\"co_occurrence_probs: {co_occurrence_probs}\")\n",
    "    \n",
    "    # Step 2: Calculate word probabilities\n",
    "    # calculate word probabilities as marginal probabilities i.e sum of probabilities across all the words\n",
    "    word_probs = co_occurrence_probs.sum(axis=1)    \n",
    "    # print(f\"word_probs: {word_probs}\")\n",
    "    \n",
    "    # Step 3: Calculate PMI matrix\n",
    "    pmi_matrix = np.zeros(Xc.shape, dtype=float)\n",
    "    # print(f\"Xc.shape:{Xc.shape}\")\n",
    "    # print(f\"pmi_matrix.shape:{pmi_matrix.shape}\")\n",
    "    \n",
    "    for i in range(Xc.shape[0]):\n",
    "        for j in range(Xc.shape[1]):\n",
    "            p_a = word_probs[i]\n",
    "            p_b = word_probs[j]\n",
    "            p_ab = co_occurrence_probs[i, j]\n",
    "            if p_a > 0 and p_b > 0 and p_ab > 0:\n",
    "                pmi_matrix[i][j] = math.log2(p_ab / (p_a * p_b))\n",
    "\n",
    "    return pmi_matrix\n",
    "\n",
    "def optimized_convert_coccurence_matrix_to_pmi(Xc):\n",
    "    total_word_occurrences = np.sum(Xc)\n",
    "\n",
    "    # Step 1: Calculate co-occurrence probabilities\n",
    "    co_occurrence_probs = Xc / total_word_occurrences\n",
    "    # print(f\"co_occurrence_probs: {co_occurrence_probs}\")\n",
    "\n",
    "    # Step 2: Calculate word probabilities\n",
    "    p_xi = co_occurrence_probs.sum(axis=1)\n",
    "    \n",
    "    # divide each row by the sum of the row\n",
    "    co_occurrence_probs = co_occurrence_probs / p_xi\n",
    "\n",
    "    # divide each column by the sum of the column\n",
    "    co_occurrence_probs = co_occurrence_probs / p_xi.T\n",
    "    # take log2 of the matrix\n",
    "    co_occurrence_probs = np.log2(co_occurrence_probs)\n",
    "    return co_occurrence_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a672bea37fa65249",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-21T05:05:33.458064027Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book' 'cat' 'good' 'shit' 'this']\n",
      "[[0 0 0 0 1]\n",
      " [0 0 2 1 1]\n",
      " [0 2 0 1 1]\n",
      " [0 1 1 0 0]\n",
      " [1 1 1 0 0]]\n",
      "total_word_occurrences:14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.000000, 0.000000, 0.000000, 0.000000, 2.222392],\n",
       "       [0.000000, 0.000000, 0.807355, 0.807355, 0.222392],\n",
       "       [0.000000, 0.807355, 0.000000, 0.807355, 0.222392],\n",
       "       [0.000000, 0.807355, 0.807355, 0.000000, 0.000000],\n",
       "       [2.222392, 0.222392, 0.222392, 0.000000, 0.000000]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example PMI calculation of small data\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import one hot encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "docs = ['this this this book',\n",
    "        'this cat good',\n",
    "        'cat good shit']\n",
    "\n",
    "\n",
    "\n",
    "count_model = CountVectorizer(binary=True) \n",
    "document_term_matrix = count_model.fit_transform(docs)\n",
    "print(count_model.get_feature_names_out())\n",
    "# print(X)\n",
    "term_term_matrix = (document_term_matrix.T * document_term_matrix) # this is co-occurrence matrix in sparse csr format\n",
    "term_term_matrix.setdiag(0) # sometimes you want to fill same word cooccurence to 0\n",
    "print(term_term_matrix.todense())\n",
    "convert_coccurence_matrix_to_pmi(term_term_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "793c0ecd-84cc-4c76-8881-f1e46594383d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term_term_matrix.shape:(16045, 16045)\n",
      "['a' 'abandon' 'abandonest' ... 'zealously' 'zodiac' 'zone']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankush/anaconda3/envs/nlp-notebooks/lib/python3.7/site-packages/ipykernel_launcher.py:51: RuntimeWarning: divide by zero encountered in log2\n",
      "/home/ankush/anaconda3/envs/nlp-notebooks/lib/python3.7/site-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "krishna: ['wonderful', 'vasudeva', 'draupadi', 'compose', 'arjuna', 'pandava', 'vyasa', 'pandavas', 'poet', 'aranya']\n",
      "mace: ['discus', 'host', 'huge', 'shower', 'roar', 'sword', 'bhima', 'fight', 'tempestuous', 'rush']\n",
      "bhishma: ['yudhishthira', 'pandu', 'kurus', 'vyasa', 'pandava', 'bharata', 'history', 'prince', 'arrow', 'drona']\n",
      "drona: ['kripa', 'aswatthaman', 'arjuna', 'karna', 'kauravas', 'duryodhana', 'pandava', 'pandavas', 'chariot', 'panchala']\n",
      "pandu: ['kunti', 'vaisampayana', 'pandava', 'kurus', 'hath', 'dhritarashtra', 'bhimasena', 'kuru', 'arjuna', 'vidura']\n"
     ]
    }
   ],
   "source": [
    "# Calculate word vectors using PMI wighting\n",
    "#document: section\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "# print(section_wise_corpus[0])\n",
    "# get document-term matrix via Count vectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(tokenizer=SpacyLemmaTokenizer())\n",
    "SECTION_WISE_CORPUS = get_sections_from_text(\"./data/mahabharat_gutenberg.jsonl\")\n",
    "\n",
    "s_time = time.time()\n",
    "document_term_matrix = count_vectorizer.fit_transform(SECTION_WISE_CORPUS)\n",
    "print(f\"fit_transform took: {time.time()- s_time}\")\n",
    "\n",
    "# get term-term matrix\n",
    "term_term_matrix = (document_term_matrix.T * document_term_matrix) \n",
    "term_term_matrix.setdiag(0)\n",
    "\n",
    "print(f\"term_term_matrix.shape:{term_term_matrix.shape}\")\n",
    "\n",
    "# list all features ordered by frequency\n",
    "print(count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# convert couccurence_matrix to PMI matrix\n",
    "term_pmi_matrix = optimized_convert_coccurence_matrix_to_pmi(term_term_matrix)\n",
    "term_pmi_matrix[term_pmi_matrix == -np.inf] = 0\n",
    "\n",
    "# get similarity of words\n",
    "\n",
    "section_doc_pmi_similarity_matrix = cosine_similarity(term_pmi_matrix)\n",
    "# print(section_doc_pmi_similarity_matrix.shape)\n",
    "\n",
    "for word in [\"krishna\", \"mace\", \"bhishma\", \"drona\", \"pandu\"]:\n",
    "    print(f\"{word}: {get_top_similar_words(word, section_doc_pmi_similarity_matrix, count_vectorizer)}\")\n",
    "# get_top_similar_words(\"\", similarity_matrix, vectorizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8170020d-b089-4fff-8103-411f1985d4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document_matrix formation took:509.12226700782776 secs\n",
      "term_term_matrix.shape:(15347, 15347)\n",
      "['abandon' 'abandonest' 'abandonment' ... 'zealously' 'zodiac' 'zone']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankush/anaconda3/envs/nlp-notebooks/lib/python3.7/site-packages/ipykernel_launcher.py:51: RuntimeWarning: divide by zero encountered in log2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert_coccurence_matrix_to_pmi took:4.455509424209595 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankush/anaconda3/envs/nlp-notebooks/lib/python3.7/site-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity took:16.46736788749695 secs\n",
      "krishna: ['vasudeva', 'arjuna', 'pandava', 'madhava', 'draupadi', 'subhadra', 'hero', 'pandavas', 'vrishni', 'khandava']\n",
      "mace: ['discus', 'going', 'swah', 'pubescen', 'abdoman', 'baffle', 'trice', 'sugriva', 'overspread', 'lxiv']\n",
      "bhishma: ['kurus', 'address', 'yudhishthira', 'santanu', 'drona', 'kuru', 'arjuna', 'continue', 'bharata', 'old']\n",
      "drona: ['kripa', 'arjuna', 'aswatthaman', 'karna', 'warrior', 'duryodhana', 'encounter', 'weapon', 'arrow', 'fight']\n",
      "pandu: ['kunti', 'pandava', 'vaisampayana', 'arjuna', 'dhritarashtra', 'kurus', 'kuru', 'yudhishthira', 'duryodhana', 'pritha']\n"
     ]
    }
   ],
   "source": [
    "# Calculate word vectors using PMI wighting\n",
    "#document: paragraph\n",
    "import time\n",
    "import ujson\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# get document-term matrix via Count vectorizer\n",
    "count_vectorizer = CountVectorizer(tokenizer=SpacyLemmaTokenizer()) \n",
    "s_time = time.time()\n",
    "PARA_WISE_CORPUS = get_paragraphs_from_text(\"./data/mahabharat_gutenberg.jsonl\")\n",
    "document_term_matrix = count_vectorizer.fit_transform(PARA_WISE_CORPUS)\n",
    "print(f\"document_matrix formation took:{time.time() - s_time} secs\")\n",
    "\n",
    "# get term-term matrix\n",
    "term_term_matrix = (document_term_matrix.T * document_term_matrix)\n",
    "term_term_matrix.setdiag(0)\n",
    "\n",
    "print(f\"term_term_matrix.shape:{term_term_matrix.shape}\")\n",
    "\n",
    "# list all features ordered by frequency\n",
    "print(count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# convert couccurence_matrix to PMI matrix\n",
    "s_time = time.time()\n",
    "term_pmi_matrix = optimized_convert_coccurence_matrix_to_pmi(term_term_matrix)\n",
    "term_pmi_matrix[term_pmi_matrix == -np.inf] = 0\n",
    "print(f\"convert_coccurence_matrix_to_pmi took:{time.time() - s_time} secs\")\n",
    "\n",
    "\n",
    "s_time = time.time()\n",
    "para_doc_pmi_similarity_matrix = cosine_similarity(term_pmi_matrix)\n",
    "print(f\"cosine_similarity took:{time.time() - s_time} secs\")\n",
    "\n",
    "for word in [\"krishna\", \"mace\", \"bhishma\", \"drona\", \"pandu\"]:\n",
    "    print(f\"{word}: {get_top_similar_words(word, para_doc_pmi_similarity_matrix, count_vectorizer)}\")\n",
    "# get_top_similar_words(\"\", similarity_matrix, vectorizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3926b2-d04b-40d8-a632-336767d74f23",
   "metadata": {},
   "source": [
    "# References:\n",
    "1. [(Chapter 6- Vector Semantics and Embeddings) in Speech and Language Processing. Daniel Jurafsky & James H. Martin.](https://web.stanford.edu/~jurafsky/slp3/6.pdf)\n",
    "2. [sklearn.feature_extraction.text](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
