{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:14:38.054774748Z",
     "start_time": "2023-08-30T07:14:37.255421179Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "device = get_default_device()\n",
    "print(device)\n",
    "\n",
    "def to_device(data, device):\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = [\n",
    "#     'he is a king',\n",
    "#     'she is a queen',\n",
    "#     'he is a man',\n",
    "#     'she is a woman',\n",
    "#     'warsaw is poland capital',\n",
    "#     'berlin is germany capital',\n",
    "#     'paris is france capital',   \n",
    "# ]\n",
    "\n",
    "import re\n",
    "\n",
    "al_regex = re.compile(r\"[^a-zA-Z]\")\n",
    "\n",
    "\n",
    "class MBCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = \"data/mahabharat_gutenberg_lemmatized_sents.txt\"\n",
    "        with open(corpus_path) as fp:\n",
    "            for line in fp.readlines():\n",
    "                tokens = line.split()\n",
    "                tokens = [al_regex.sub('', token) for token in tokens]\n",
    "                yield tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary_size:16456\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mb_sents = MBCorpus()\n",
    "vocabulary = []\n",
    "for sentence in mb_sents:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(f\"vocabulary_size:{vocabulary_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).type(torch.cuda.FloatTensor)\n",
    "    x[word_idx] = 1.0\n",
    "    \n",
    "    return to_device(x, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3342, -0.1637,  1.7855,  1.5707,  0.9831],\n",
      "        [-1.7907,  0.5738, -1.0215, -0.5571, -0.4025],\n",
      "        [-1.9868,  0.1662,  1.4823,  0.5362,  1.5519]], requires_grad=True)\n",
      "tensor([1, 3, 0])\n"
     ]
    }
   ],
   "source": [
    "# Example of target with class indices\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randint(5, (3,), dtype=torch.int64)\n",
    "loss = F.cross_entropy(input, target)\n",
    "loss.backward()\n",
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "print(input)\n",
    "print(target)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "loss = F.cross_entropy(input, target)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T07:14:29.485418854Z",
     "start_time": "2023-08-30T07:14:29.479921397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 0: 1531889.403954506\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 1: 1531878.3976774216\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 2: 1531815.7917280197\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 3: 1531801.3615150452\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 4: 1531763.4347925186\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 5: 1531759.412569046\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 6: 1531759.330965042\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 7: 1531759.262793541\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 8: 1531759.2134475708\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 9: 1531759.166144371\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 10: 1531759.1276130676\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 11: 1531759.0484838486\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 12: 1531756.5032157898\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 13: 1531753.2653074265\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 14: 1531753.2647266388\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 15: 1531753.2284965515\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 16: 1531753.2371263504\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 17: 1531753.2191448212\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 18: 1531753.224225998\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 19: 1531753.2020702362\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 20: 1531753.19337368\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 21: 1531753.191701889\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 22: 1531753.158027649\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 23: 1531753.140501976\n",
      "processing 100000th sentence\n",
      "processing 200000th sentence\n",
      "processing 300000th sentence\n",
      "Loss at epo 24: 1531752.846367836\n"
     ]
    }
   ],
   "source": [
    "embedding_dims = 100\n",
    "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).type(torch.cuda.FloatTensor), requires_grad=True)\n",
    "# to_device(W1, device)\n",
    "W2 = Variable(torch.randn(vocabulary_size, embedding_dims).type(torch.cuda.FloatTensor), requires_grad=True)\n",
    "# to_device(W2, device)\n",
    "num_epochs = 101\n",
    "learning_rate = 0.1\n",
    "window_size = 2\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    # for each sentence\n",
    "    for sent_idx, sentence in enumerate(mb_sents):\n",
    "        if sent_idx!=0 and sent_idx%100000==0:            \n",
    "            print(f\"processing {sent_idx}th sentence\")\n",
    "            # break\n",
    "            \n",
    "        for idx, target_word in enumerate(sentence):\n",
    "            target_idx = word2idx[target_word]\n",
    "            context = [sentence[idx] for idx in range(max(0, target_idx - window_size), min(target_idx + window_size + 1, len(sentence))) if idx != target_idx]\n",
    "                # print(f\"target_word:{target_word}, context:{context}\")            \n",
    "            if not context:\n",
    "                continue\n",
    "            context_indices = [word2idx[word] for word in context]\n",
    "            \n",
    "            for context_idx in context_indices:                \n",
    "                x = Variable(get_input_layer(target_idx))\n",
    "                # y_true = Variable(torch.from_numpy(np.array([context_idx])).long())\n",
    "                y_true = Variable(get_input_layer(context_idx))\n",
    "                # print(y_true)\n",
    "                # print(f\"W1*x: {W1.shape}*{x.shape}\")\n",
    "                z1 = torch.matmul(W1, x)\n",
    "                # print(f\"W2*z1: {W2.shape}*{z1.shape}\")\n",
    "                z2 = torch.matmul(W2, z1)\n",
    "                # make z2 look like one-hot via softmax and then setting max probability to 1\n",
    "                # z2_soft_max = F.softmax(z2, dim=0)\n",
    "                # z2_one_hot = torch.zeros_like(z2_soft_max).type(torch.cuda.FloatTensor)\n",
    "                # z2_one_hot[torch.argmax(z2_soft_max)] = 1\n",
    "                \n",
    "                \n",
    "                # calculate softmax of z2\n",
    "                sf_z2 = F.softmax(z2, dim=0)\n",
    "                # print(f\"sf_z2.shape: {sf_z2.shape}\")\n",
    "                # print(f\"y_true.shape: {y_true.shape}\")\n",
    "                # apply cross entropy loss\n",
    "                # print()\n",
    "                loss = F.cross_entropy(sf_z2, y_true) #\n",
    "                \n",
    "                \n",
    "                # print(f\"z2: {z2}\")\n",
    "                # print(f\"z2.shape: {z2.shape}\")\n",
    "                # print(f\"z2: {z2}\")\n",
    "                # log_softmax = F.log_softmax(z2, dim=0)\n",
    "                # print(f\"log_softmax: {log_softmax}\")\n",
    "                # loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "                loss_val += loss.item()\n",
    "                loss.backward()\n",
    "                W1.data -= learning_rate * W1.grad.data\n",
    "                W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "                W1.grad.data.zero_()\n",
    "                W2.grad.data.zero_()\n",
    "    print(f'Loss at epo {epo}: {loss_val}')\n",
    "        # get corresponding context embeddings\n",
    "        # for each target, context pair train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epo 0: 4.04387846807284\n",
      "Loss at epo 10: 3.6861324227814163\n",
      "Loss at epo 20: 3.4341715413544858\n",
      "Loss at epo 30: 3.2438723781279157\n",
      "Loss at epo 40: 3.098003941135747\n",
      "Loss at epo 50: 2.984993450130735\n",
      "Loss at epo 60: 2.895948580120291\n",
      "Loss at epo 70: 2.8241932072809766\n",
      "Loss at epo 80: 2.7649008263434682\n",
      "Loss at epo 90: 2.714670770721776\n",
      "Loss at epo 100: 2.6711281963757108\n"
     ]
    }
   ],
   "source": [
    "embedding_dims = 5\n",
    "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n",
    "W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n",
    "num_epochs = 101\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for data, target in idx_pairs:\n",
    "        x = Variable(get_input_layer(data)).float()\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "    \n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        loss_val += loss.item()\n",
    "        loss.backward()\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "    if epo % 10 == 0:    \n",
    "        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
